{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bcf6e9",
   "metadata": {},
   "source": [
    "# Constructing Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edce356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c5701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b75bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab821cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278fa8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = pd.read_csv(r\"train_dataset.csv\", index_col = 0)\n",
    "# val_dataset = pd.read_csv(r\"val_dataset.csv\", index_col = 0)\n",
    "# test_dataset = pd.read_csv(r\"test_dataset.csv\", index_col = 0)\n",
    "\n",
    "# features = pd.read_csv(r\"features.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf73cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_top = pd.read_csv(r\"train_dataset_top.csv\", index_col = 0)\n",
    "val_dataset_top = pd.read_csv(r\"val_dataset_top.csv\", index_col = 0)\n",
    "test_dataset_top = pd.read_csv(r\"test_dataset_top.csv\", index_col = 0)\n",
    "\n",
    "features_top = pd.read_csv(r\"features_top.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f3511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>indmom</th>\n",
       "      <th>mom1m</th>\n",
       "      <th>...</th>\n",
       "      <th>invest*dfy</th>\n",
       "      <th>invest*svar</th>\n",
       "      <th>absacc*dp_sp</th>\n",
       "      <th>absacc*ep_sp</th>\n",
       "      <th>absacc*bm_sp</th>\n",
       "      <th>absacc*ntis</th>\n",
       "      <th>absacc*tbl</th>\n",
       "      <th>absacc*tms</th>\n",
       "      <th>absacc*dfy</th>\n",
       "      <th>absacc*svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>14593</td>\n",
       "      <td>201506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.392488</td>\n",
       "      <td>-0.843512</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>0.968018</td>\n",
       "      <td>-0.718208</td>\n",
       "      <td>-0.604590</td>\n",
       "      <td>-0.119343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365988</td>\n",
       "      <td>-0.374302</td>\n",
       "      <td>-0.754788</td>\n",
       "      <td>-0.816891</td>\n",
       "      <td>-0.810511</td>\n",
       "      <td>-0.285030</td>\n",
       "      <td>-0.969760</td>\n",
       "      <td>-0.831976</td>\n",
       "      <td>-0.819573</td>\n",
       "      <td>-0.974298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>14593</td>\n",
       "      <td>201503</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>-0.373529</td>\n",
       "      <td>-0.832350</td>\n",
       "      <td>0.346878</td>\n",
       "      <td>0.971090</td>\n",
       "      <td>-0.711564</td>\n",
       "      <td>-0.659332</td>\n",
       "      <td>-0.020089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379144</td>\n",
       "      <td>-0.372207</td>\n",
       "      <td>-0.774483</td>\n",
       "      <td>-0.819861</td>\n",
       "      <td>-0.822897</td>\n",
       "      <td>-0.271672</td>\n",
       "      <td>-0.957228</td>\n",
       "      <td>-0.872915</td>\n",
       "      <td>-0.837108</td>\n",
       "      <td>-0.960373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>14593</td>\n",
       "      <td>201504</td>\n",
       "      <td>0.930708</td>\n",
       "      <td>-0.359465</td>\n",
       "      <td>-0.823821</td>\n",
       "      <td>0.354844</td>\n",
       "      <td>0.965641</td>\n",
       "      <td>-0.717428</td>\n",
       "      <td>-0.627439</td>\n",
       "      <td>-0.256641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364658</td>\n",
       "      <td>-0.377033</td>\n",
       "      <td>-0.761046</td>\n",
       "      <td>-0.813335</td>\n",
       "      <td>-0.812858</td>\n",
       "      <td>-0.275138</td>\n",
       "      <td>-0.969760</td>\n",
       "      <td>-0.853350</td>\n",
       "      <td>-0.815734</td>\n",
       "      <td>-0.985970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>14593</td>\n",
       "      <td>201505</td>\n",
       "      <td>0.921025</td>\n",
       "      <td>-0.351181</td>\n",
       "      <td>-0.818698</td>\n",
       "      <td>0.343828</td>\n",
       "      <td>0.974312</td>\n",
       "      <td>-0.720239</td>\n",
       "      <td>-0.596675</td>\n",
       "      <td>-0.189967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367983</td>\n",
       "      <td>-0.375246</td>\n",
       "      <td>-0.761733</td>\n",
       "      <td>-0.818006</td>\n",
       "      <td>-0.814626</td>\n",
       "      <td>-0.281153</td>\n",
       "      <td>-0.969760</td>\n",
       "      <td>-0.845632</td>\n",
       "      <td>-0.825331</td>\n",
       "      <td>-0.978333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>14593</td>\n",
       "      <td>201507</td>\n",
       "      <td>0.906199</td>\n",
       "      <td>-0.377502</td>\n",
       "      <td>-0.834720</td>\n",
       "      <td>0.362267</td>\n",
       "      <td>0.956196</td>\n",
       "      <td>-0.719959</td>\n",
       "      <td>-0.620920</td>\n",
       "      <td>-0.267217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.358673</td>\n",
       "      <td>-0.373687</td>\n",
       "      <td>-0.758060</td>\n",
       "      <td>-0.823116</td>\n",
       "      <td>-0.811265</td>\n",
       "      <td>-0.284888</td>\n",
       "      <td>-0.954640</td>\n",
       "      <td>-0.845632</td>\n",
       "      <td>-0.798459</td>\n",
       "      <td>-0.971669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143995</th>\n",
       "      <td>71810</td>\n",
       "      <td>201007</td>\n",
       "      <td>-0.999962</td>\n",
       "      <td>-0.417357</td>\n",
       "      <td>-0.857571</td>\n",
       "      <td>0.344090</td>\n",
       "      <td>0.388166</td>\n",
       "      <td>-0.725931</td>\n",
       "      <td>-0.489277</td>\n",
       "      <td>-0.164908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399507</td>\n",
       "      <td>-0.373848</td>\n",
       "      <td>-0.851645</td>\n",
       "      <td>-0.848859</td>\n",
       "      <td>-0.852628</td>\n",
       "      <td>-0.175964</td>\n",
       "      <td>-0.852672</td>\n",
       "      <td>-0.869472</td>\n",
       "      <td>-0.849209</td>\n",
       "      <td>-0.950183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143996</th>\n",
       "      <td>93223</td>\n",
       "      <td>201009</td>\n",
       "      <td>-0.999964</td>\n",
       "      <td>-0.354021</td>\n",
       "      <td>-0.820463</td>\n",
       "      <td>0.342438</td>\n",
       "      <td>0.546012</td>\n",
       "      <td>-0.610931</td>\n",
       "      <td>-0.529142</td>\n",
       "      <td>-0.184627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403334</td>\n",
       "      <td>-0.375843</td>\n",
       "      <td>-0.853107</td>\n",
       "      <td>-0.844911</td>\n",
       "      <td>-0.854731</td>\n",
       "      <td>-0.220364</td>\n",
       "      <td>-0.859660</td>\n",
       "      <td>-0.880233</td>\n",
       "      <td>-0.865789</td>\n",
       "      <td>-0.964673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143997</th>\n",
       "      <td>81678</td>\n",
       "      <td>201009</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.217733</td>\n",
       "      <td>-0.726060</td>\n",
       "      <td>0.373264</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>-0.640793</td>\n",
       "      <td>-0.454958</td>\n",
       "      <td>-0.384268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440531</td>\n",
       "      <td>-0.382456</td>\n",
       "      <td>-0.696140</td>\n",
       "      <td>-0.679185</td>\n",
       "      <td>-0.699499</td>\n",
       "      <td>-0.207139</td>\n",
       "      <td>-0.709696</td>\n",
       "      <td>-0.752251</td>\n",
       "      <td>-0.722374</td>\n",
       "      <td>-0.926923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143998</th>\n",
       "      <td>81912</td>\n",
       "      <td>201009</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.569619</td>\n",
       "      <td>-0.929223</td>\n",
       "      <td>0.362420</td>\n",
       "      <td>0.459664</td>\n",
       "      <td>-0.714866</td>\n",
       "      <td>-0.510021</td>\n",
       "      <td>-0.415721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.629789</td>\n",
       "      <td>-0.416103</td>\n",
       "      <td>-0.979467</td>\n",
       "      <td>-0.978321</td>\n",
       "      <td>-0.979694</td>\n",
       "      <td>-0.231010</td>\n",
       "      <td>-0.980383</td>\n",
       "      <td>-0.983258</td>\n",
       "      <td>-0.981239</td>\n",
       "      <td>-0.995062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143999</th>\n",
       "      <td>85424</td>\n",
       "      <td>201009</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.156725</td>\n",
       "      <td>-0.677363</td>\n",
       "      <td>0.295842</td>\n",
       "      <td>0.619961</td>\n",
       "      <td>-0.557295</td>\n",
       "      <td>-0.454958</td>\n",
       "      <td>-0.180100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248191</td>\n",
       "      <td>-0.348261</td>\n",
       "      <td>-0.892347</td>\n",
       "      <td>-0.886340</td>\n",
       "      <td>-0.893537</td>\n",
       "      <td>-0.223670</td>\n",
       "      <td>-0.897149</td>\n",
       "      <td>-0.912226</td>\n",
       "      <td>-0.901641</td>\n",
       "      <td>-0.974110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72000 rows × 914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno    DATE     mvel1      beta    betasq     chmom    dolvol  \\\n",
       "165      14593  201506  1.000000 -0.392488 -0.843512  0.319507  0.968018   \n",
       "167      14593  201503  0.993421 -0.373529 -0.832350  0.346878  0.971090   \n",
       "176      14593  201504  0.930708 -0.359465 -0.823821  0.354844  0.965641   \n",
       "178      14593  201505  0.921025 -0.351181 -0.818698  0.343828  0.974312   \n",
       "181      14593  201507  0.906199 -0.377502 -0.834720  0.362267  0.956196   \n",
       "...        ...     ...       ...       ...       ...       ...       ...   \n",
       "143995   71810  201007 -0.999962 -0.417357 -0.857571  0.344090  0.388166   \n",
       "143996   93223  201009 -0.999964 -0.354021 -0.820463  0.342438  0.546012   \n",
       "143997   81678  201009 -0.999987 -0.217733 -0.726060  0.373264  0.491475   \n",
       "143998   81912  201009 -0.999996 -0.569619 -0.929223  0.362420  0.459664   \n",
       "143999   85424  201009 -1.000000 -0.156725 -0.677363  0.295842  0.619961   \n",
       "\n",
       "         idiovol    indmom     mom1m  ...  invest*dfy  invest*svar  \\\n",
       "165    -0.718208 -0.604590 -0.119343  ...   -0.365988    -0.374302   \n",
       "167    -0.711564 -0.659332 -0.020089  ...   -0.379144    -0.372207   \n",
       "176    -0.717428 -0.627439 -0.256641  ...   -0.364658    -0.377033   \n",
       "178    -0.720239 -0.596675 -0.189967  ...   -0.367983    -0.375246   \n",
       "181    -0.719959 -0.620920 -0.267217  ...   -0.358673    -0.373687   \n",
       "...          ...       ...       ...  ...         ...          ...   \n",
       "143995 -0.725931 -0.489277 -0.164908  ...   -0.399507    -0.373848   \n",
       "143996 -0.610931 -0.529142 -0.184627  ...   -0.403334    -0.375843   \n",
       "143997 -0.640793 -0.454958 -0.384268  ...   -0.440531    -0.382456   \n",
       "143998 -0.714866 -0.510021 -0.415721  ...   -0.629789    -0.416103   \n",
       "143999 -0.557295 -0.454958 -0.180100  ...   -0.248191    -0.348261   \n",
       "\n",
       "        absacc*dp_sp  absacc*ep_sp  absacc*bm_sp  absacc*ntis  absacc*tbl  \\\n",
       "165        -0.754788     -0.816891     -0.810511    -0.285030   -0.969760   \n",
       "167        -0.774483     -0.819861     -0.822897    -0.271672   -0.957228   \n",
       "176        -0.761046     -0.813335     -0.812858    -0.275138   -0.969760   \n",
       "178        -0.761733     -0.818006     -0.814626    -0.281153   -0.969760   \n",
       "181        -0.758060     -0.823116     -0.811265    -0.284888   -0.954640   \n",
       "...              ...           ...           ...          ...         ...   \n",
       "143995     -0.851645     -0.848859     -0.852628    -0.175964   -0.852672   \n",
       "143996     -0.853107     -0.844911     -0.854731    -0.220364   -0.859660   \n",
       "143997     -0.696140     -0.679185     -0.699499    -0.207139   -0.709696   \n",
       "143998     -0.979467     -0.978321     -0.979694    -0.231010   -0.980383   \n",
       "143999     -0.892347     -0.886340     -0.893537    -0.223670   -0.897149   \n",
       "\n",
       "        absacc*tms  absacc*dfy  absacc*svar  \n",
       "165      -0.831976   -0.819573    -0.974298  \n",
       "167      -0.872915   -0.837108    -0.960373  \n",
       "176      -0.853350   -0.815734    -0.985970  \n",
       "178      -0.845632   -0.825331    -0.978333  \n",
       "181      -0.845632   -0.798459    -0.971669  \n",
       "...            ...         ...          ...  \n",
       "143995   -0.869472   -0.849209    -0.950183  \n",
       "143996   -0.880233   -0.865789    -0.964673  \n",
       "143997   -0.752251   -0.722374    -0.926923  \n",
       "143998   -0.983258   -0.981239    -0.995062  \n",
       "143999   -0.912226   -0.901641    -0.974110  \n",
       "\n",
       "[72000 rows x 914 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffed198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>indmom</th>\n",
       "      <th>mom1m</th>\n",
       "      <th>...</th>\n",
       "      <th>invest*dfy</th>\n",
       "      <th>invest*svar</th>\n",
       "      <th>absacc*dp_sp</th>\n",
       "      <th>absacc*ep_sp</th>\n",
       "      <th>absacc*bm_sp</th>\n",
       "      <th>absacc*ntis</th>\n",
       "      <th>absacc*tbl</th>\n",
       "      <th>absacc*tms</th>\n",
       "      <th>absacc*dfy</th>\n",
       "      <th>absacc*svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>14593</td>\n",
       "      <td>201712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427168</td>\n",
       "      <td>-0.866469</td>\n",
       "      <td>-0.188808</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>-0.694169</td>\n",
       "      <td>-0.239114</td>\n",
       "      <td>-0.117523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405542</td>\n",
       "      <td>-0.438850</td>\n",
       "      <td>-0.832627</td>\n",
       "      <td>-0.812921</td>\n",
       "      <td>-0.856685</td>\n",
       "      <td>0.880388</td>\n",
       "      <td>-0.788533</td>\n",
       "      <td>-0.895394</td>\n",
       "      <td>-0.878329</td>\n",
       "      <td>-0.985898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>14593</td>\n",
       "      <td>201711</td>\n",
       "      <td>0.967175</td>\n",
       "      <td>-0.422871</td>\n",
       "      <td>-0.864160</td>\n",
       "      <td>-0.109102</td>\n",
       "      <td>0.974043</td>\n",
       "      <td>-0.694448</td>\n",
       "      <td>-0.177485</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406073</td>\n",
       "      <td>-0.436860</td>\n",
       "      <td>-0.827774</td>\n",
       "      <td>-0.808139</td>\n",
       "      <td>-0.854047</td>\n",
       "      <td>0.926637</td>\n",
       "      <td>-0.802951</td>\n",
       "      <td>-0.882533</td>\n",
       "      <td>-0.880043</td>\n",
       "      <td>-0.979472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>14593</td>\n",
       "      <td>201709</td>\n",
       "      <td>0.919836</td>\n",
       "      <td>-0.421542</td>\n",
       "      <td>-0.863441</td>\n",
       "      <td>-0.111741</td>\n",
       "      <td>0.964810</td>\n",
       "      <td>-0.689649</td>\n",
       "      <td>-0.259303</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407665</td>\n",
       "      <td>-0.438615</td>\n",
       "      <td>-0.825134</td>\n",
       "      <td>-0.806526</td>\n",
       "      <td>-0.841883</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>-0.834992</td>\n",
       "      <td>-0.866241</td>\n",
       "      <td>-0.885184</td>\n",
       "      <td>-0.985139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14593</td>\n",
       "      <td>201706</td>\n",
       "      <td>0.804640</td>\n",
       "      <td>-0.432036</td>\n",
       "      <td>-0.869060</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>0.954546</td>\n",
       "      <td>-0.693470</td>\n",
       "      <td>-0.189259</td>\n",
       "      <td>-0.031598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406603</td>\n",
       "      <td>-0.435210</td>\n",
       "      <td>-0.821811</td>\n",
       "      <td>-0.804613</td>\n",
       "      <td>-0.834066</td>\n",
       "      <td>0.941129</td>\n",
       "      <td>-0.843002</td>\n",
       "      <td>-0.862812</td>\n",
       "      <td>-0.881756</td>\n",
       "      <td>-0.974145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>14593</td>\n",
       "      <td>201710</td>\n",
       "      <td>0.790043</td>\n",
       "      <td>-0.430363</td>\n",
       "      <td>-0.868173</td>\n",
       "      <td>-0.161052</td>\n",
       "      <td>0.969809</td>\n",
       "      <td>-0.696905</td>\n",
       "      <td>-0.305205</td>\n",
       "      <td>-0.263456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405011</td>\n",
       "      <td>-0.439314</td>\n",
       "      <td>-0.828031</td>\n",
       "      <td>-0.809076</td>\n",
       "      <td>-0.848458</td>\n",
       "      <td>0.925520</td>\n",
       "      <td>-0.828584</td>\n",
       "      <td>-0.867956</td>\n",
       "      <td>-0.876615</td>\n",
       "      <td>-0.987397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137476</th>\n",
       "      <td>14539</td>\n",
       "      <td>201603</td>\n",
       "      <td>-0.999930</td>\n",
       "      <td>-0.361609</td>\n",
       "      <td>-0.829087</td>\n",
       "      <td>-0.116573</td>\n",
       "      <td>0.637090</td>\n",
       "      <td>-0.875453</td>\n",
       "      <td>-0.565556</td>\n",
       "      <td>-0.167531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409703</td>\n",
       "      <td>-0.432228</td>\n",
       "      <td>-0.883076</td>\n",
       "      <td>-0.885341</td>\n",
       "      <td>-0.880158</td>\n",
       "      <td>0.914354</td>\n",
       "      <td>-0.972115</td>\n",
       "      <td>-0.902735</td>\n",
       "      <td>-0.865259</td>\n",
       "      <td>-0.955821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137485</th>\n",
       "      <td>58975</td>\n",
       "      <td>201602</td>\n",
       "      <td>-0.999932</td>\n",
       "      <td>-0.070517</td>\n",
       "      <td>-0.607461</td>\n",
       "      <td>-0.026761</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>-0.663091</td>\n",
       "      <td>-0.872612</td>\n",
       "      <td>-0.331486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319783</td>\n",
       "      <td>-0.364774</td>\n",
       "      <td>-0.722062</td>\n",
       "      <td>-0.726339</td>\n",
       "      <td>-0.729357</td>\n",
       "      <td>0.800130</td>\n",
       "      <td>-0.933284</td>\n",
       "      <td>-0.785755</td>\n",
       "      <td>-0.682305</td>\n",
       "      <td>-0.798103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137511</th>\n",
       "      <td>31500</td>\n",
       "      <td>201602</td>\n",
       "      <td>-0.999938</td>\n",
       "      <td>-0.100652</td>\n",
       "      <td>-0.634620</td>\n",
       "      <td>-0.163056</td>\n",
       "      <td>0.525813</td>\n",
       "      <td>-0.717380</td>\n",
       "      <td>-0.520198</td>\n",
       "      <td>-0.350071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440848</td>\n",
       "      <td>-0.441711</td>\n",
       "      <td>-0.697071</td>\n",
       "      <td>-0.701732</td>\n",
       "      <td>-0.705021</td>\n",
       "      <td>0.782158</td>\n",
       "      <td>-0.927285</td>\n",
       "      <td>-0.766491</td>\n",
       "      <td>-0.653740</td>\n",
       "      <td>-0.779950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137565</th>\n",
       "      <td>75041</td>\n",
       "      <td>201603</td>\n",
       "      <td>-0.999956</td>\n",
       "      <td>-0.459713</td>\n",
       "      <td>-0.883315</td>\n",
       "      <td>-0.025804</td>\n",
       "      <td>0.386708</td>\n",
       "      <td>-0.749738</td>\n",
       "      <td>-0.479529</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365305</td>\n",
       "      <td>-0.417671</td>\n",
       "      <td>-0.659705</td>\n",
       "      <td>-0.666299</td>\n",
       "      <td>-0.651212</td>\n",
       "      <td>0.750736</td>\n",
       "      <td>-0.918845</td>\n",
       "      <td>-0.716920</td>\n",
       "      <td>-0.607851</td>\n",
       "      <td>-0.871421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137746</th>\n",
       "      <td>91321</td>\n",
       "      <td>201603</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.753481</td>\n",
       "      <td>0.512384</td>\n",
       "      <td>0.104888</td>\n",
       "      <td>0.506756</td>\n",
       "      <td>-0.341644</td>\n",
       "      <td>-0.826768</td>\n",
       "      <td>0.790844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314977</td>\n",
       "      <td>-0.401169</td>\n",
       "      <td>-0.844611</td>\n",
       "      <td>-0.847622</td>\n",
       "      <td>-0.840733</td>\n",
       "      <td>0.886178</td>\n",
       "      <td>-0.962942</td>\n",
       "      <td>-0.870737</td>\n",
       "      <td>-0.820933</td>\n",
       "      <td>-0.941287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno    DATE     mvel1      beta    betasq     chmom    dolvol  \\\n",
       "109      14593  201712  1.000000 -0.427168 -0.866469 -0.188808  0.999985   \n",
       "116      14593  201711  0.967175 -0.422871 -0.864160 -0.109102  0.974043   \n",
       "129      14593  201709  0.919836 -0.421542 -0.863441 -0.111741  0.964810   \n",
       "147      14593  201706  0.804640 -0.432036 -0.869060  0.063186  0.954546   \n",
       "149      14593  201710  0.790043 -0.430363 -0.868173 -0.161052  0.969809   \n",
       "...        ...     ...       ...       ...       ...       ...       ...   \n",
       "137476   14539  201603 -0.999930 -0.361609 -0.829087 -0.116573  0.637090   \n",
       "137485   58975  201602 -0.999932 -0.070517 -0.607461 -0.026761  0.562158   \n",
       "137511   31500  201602 -0.999938 -0.100652 -0.634620 -0.163056  0.525813   \n",
       "137565   75041  201603 -0.999956 -0.459713 -0.883315 -0.025804  0.386708   \n",
       "137746   91321  201603 -1.000000  0.753481  0.512384  0.104888  0.506756   \n",
       "\n",
       "         idiovol    indmom     mom1m  ...  invest*dfy  invest*svar  \\\n",
       "109    -0.694169 -0.239114 -0.117523  ...   -0.405542    -0.438850   \n",
       "116    -0.694448 -0.177485  0.020920  ...   -0.406073    -0.436860   \n",
       "129    -0.689649 -0.259303  0.039203  ...   -0.407665    -0.438615   \n",
       "147    -0.693470 -0.189259 -0.031598  ...   -0.406603    -0.435210   \n",
       "149    -0.696905 -0.305205 -0.263456  ...   -0.405011    -0.439314   \n",
       "...          ...       ...       ...  ...         ...          ...   \n",
       "137476 -0.875453 -0.565556 -0.167531  ...   -0.409703    -0.432228   \n",
       "137485 -0.663091 -0.872612 -0.331486  ...   -0.319783    -0.364774   \n",
       "137511 -0.717380 -0.520198 -0.350071  ...   -0.440848    -0.441711   \n",
       "137565 -0.749738 -0.479529  0.006529  ...   -0.365305    -0.417671   \n",
       "137746 -0.341644 -0.826768  0.790844  ...   -0.314977    -0.401169   \n",
       "\n",
       "        absacc*dp_sp  absacc*ep_sp  absacc*bm_sp  absacc*ntis  absacc*tbl  \\\n",
       "109        -0.832627     -0.812921     -0.856685     0.880388   -0.788533   \n",
       "116        -0.827774     -0.808139     -0.854047     0.926637   -0.802951   \n",
       "129        -0.825134     -0.806526     -0.841883     0.933300   -0.834992   \n",
       "147        -0.821811     -0.804613     -0.834066     0.941129   -0.843002   \n",
       "149        -0.828031     -0.809076     -0.848458     0.925520   -0.828584   \n",
       "...              ...           ...           ...          ...         ...   \n",
       "137476     -0.883076     -0.885341     -0.880158     0.914354   -0.972115   \n",
       "137485     -0.722062     -0.726339     -0.729357     0.800130   -0.933284   \n",
       "137511     -0.697071     -0.701732     -0.705021     0.782158   -0.927285   \n",
       "137565     -0.659705     -0.666299     -0.651212     0.750736   -0.918845   \n",
       "137746     -0.844611     -0.847622     -0.840733     0.886178   -0.962942   \n",
       "\n",
       "        absacc*tms  absacc*dfy  absacc*svar  \n",
       "109      -0.895394   -0.878329    -0.985898  \n",
       "116      -0.882533   -0.880043    -0.979472  \n",
       "129      -0.866241   -0.885184    -0.985139  \n",
       "147      -0.862812   -0.881756    -0.974145  \n",
       "149      -0.867956   -0.876615    -0.987397  \n",
       "...            ...         ...          ...  \n",
       "137476   -0.902735   -0.865259    -0.955821  \n",
       "137485   -0.785755   -0.682305    -0.798103  \n",
       "137511   -0.766491   -0.653740    -0.779950  \n",
       "137565   -0.716920   -0.607851    -0.871421  \n",
       "137746   -0.870737   -0.820933    -0.941287  \n",
       "\n",
       "[24000 rows x 914 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a7f0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>indmom</th>\n",
       "      <th>mom1m</th>\n",
       "      <th>...</th>\n",
       "      <th>invest*dfy</th>\n",
       "      <th>invest*svar</th>\n",
       "      <th>absacc*dp_sp</th>\n",
       "      <th>absacc*ep_sp</th>\n",
       "      <th>absacc*bm_sp</th>\n",
       "      <th>absacc*ntis</th>\n",
       "      <th>absacc*tbl</th>\n",
       "      <th>absacc*tms</th>\n",
       "      <th>absacc*dfy</th>\n",
       "      <th>absacc*svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14593</td>\n",
       "      <td>202112</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.517544</td>\n",
       "      <td>-0.917058</td>\n",
       "      <td>0.083234</td>\n",
       "      <td>0.935369</td>\n",
       "      <td>-0.766950</td>\n",
       "      <td>-0.369577</td>\n",
       "      <td>-0.175821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337816</td>\n",
       "      <td>-0.407181</td>\n",
       "      <td>-0.870484</td>\n",
       "      <td>-0.809642</td>\n",
       "      <td>-0.864945</td>\n",
       "      <td>0.345191</td>\n",
       "      <td>-0.994172</td>\n",
       "      <td>-0.563798</td>\n",
       "      <td>-0.920009</td>\n",
       "      <td>-0.988168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14593</td>\n",
       "      <td>202109</td>\n",
       "      <td>0.850689</td>\n",
       "      <td>-0.513482</td>\n",
       "      <td>-0.915312</td>\n",
       "      <td>0.090737</td>\n",
       "      <td>0.932078</td>\n",
       "      <td>-0.755616</td>\n",
       "      <td>-0.124235</td>\n",
       "      <td>-0.242029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335102</td>\n",
       "      <td>-0.408639</td>\n",
       "      <td>-0.859405</td>\n",
       "      <td>-0.813324</td>\n",
       "      <td>-0.854991</td>\n",
       "      <td>0.361192</td>\n",
       "      <td>-0.996115</td>\n",
       "      <td>-0.574823</td>\n",
       "      <td>-0.913856</td>\n",
       "      <td>-0.993453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10107</td>\n",
       "      <td>202111</td>\n",
       "      <td>0.835936</td>\n",
       "      <td>-0.613520</td>\n",
       "      <td>-0.953011</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.942327</td>\n",
       "      <td>-0.801956</td>\n",
       "      <td>-0.176537</td>\n",
       "      <td>-0.100240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329061</td>\n",
       "      <td>-0.408331</td>\n",
       "      <td>-0.893346</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>-0.886988</td>\n",
       "      <td>0.341805</td>\n",
       "      <td>-0.996143</td>\n",
       "      <td>-0.592870</td>\n",
       "      <td>-0.935504</td>\n",
       "      <td>-0.995051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10107</td>\n",
       "      <td>202112</td>\n",
       "      <td>0.830225</td>\n",
       "      <td>-0.612965</td>\n",
       "      <td>-0.952832</td>\n",
       "      <td>0.055599</td>\n",
       "      <td>0.935369</td>\n",
       "      <td>-0.804418</td>\n",
       "      <td>-0.335381</td>\n",
       "      <td>-0.288292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329728</td>\n",
       "      <td>-0.406433</td>\n",
       "      <td>-0.897154</td>\n",
       "      <td>-0.848841</td>\n",
       "      <td>-0.892756</td>\n",
       "      <td>0.328896</td>\n",
       "      <td>-0.995372</td>\n",
       "      <td>-0.603814</td>\n",
       "      <td>-0.936481</td>\n",
       "      <td>-0.990605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14593</td>\n",
       "      <td>202111</td>\n",
       "      <td>0.812219</td>\n",
       "      <td>-0.513624</td>\n",
       "      <td>-0.915374</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.942327</td>\n",
       "      <td>-0.764305</td>\n",
       "      <td>-0.243932</td>\n",
       "      <td>-0.224848</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337273</td>\n",
       "      <td>-0.408725</td>\n",
       "      <td>-0.865688</td>\n",
       "      <td>-0.808870</td>\n",
       "      <td>-0.857681</td>\n",
       "      <td>0.361447</td>\n",
       "      <td>-0.995143</td>\n",
       "      <td>-0.550015</td>\n",
       "      <td>-0.918778</td>\n",
       "      <td>-0.993767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135687</th>\n",
       "      <td>60943</td>\n",
       "      <td>202004</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.216641</td>\n",
       "      <td>-0.738566</td>\n",
       "      <td>-0.052503</td>\n",
       "      <td>0.530406</td>\n",
       "      <td>-0.803309</td>\n",
       "      <td>-0.627077</td>\n",
       "      <td>-0.622984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282093</td>\n",
       "      <td>-0.392161</td>\n",
       "      <td>-0.884515</td>\n",
       "      <td>-0.903855</td>\n",
       "      <td>-0.884515</td>\n",
       "      <td>0.230580</td>\n",
       "      <td>-0.992493</td>\n",
       "      <td>-0.718565</td>\n",
       "      <td>-0.884515</td>\n",
       "      <td>-0.962895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135744</th>\n",
       "      <td>18770</td>\n",
       "      <td>202004</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.317152</td>\n",
       "      <td>-0.809281</td>\n",
       "      <td>-0.034219</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>-0.802412</td>\n",
       "      <td>-0.585561</td>\n",
       "      <td>-0.382677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333369</td>\n",
       "      <td>-0.402463</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.900766</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>0.229440</td>\n",
       "      <td>-0.992252</td>\n",
       "      <td>-0.717294</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.961703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135768</th>\n",
       "      <td>18769</td>\n",
       "      <td>202004</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.317152</td>\n",
       "      <td>-0.809281</td>\n",
       "      <td>-0.034219</td>\n",
       "      <td>0.453554</td>\n",
       "      <td>-0.802412</td>\n",
       "      <td>-0.585561</td>\n",
       "      <td>-0.299046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333369</td>\n",
       "      <td>-0.402463</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.900766</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>0.229440</td>\n",
       "      <td>-0.992252</td>\n",
       "      <td>-0.717294</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.961703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135818</th>\n",
       "      <td>90550</td>\n",
       "      <td>202004</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.304787</td>\n",
       "      <td>-0.801182</td>\n",
       "      <td>-0.022320</td>\n",
       "      <td>0.479126</td>\n",
       "      <td>-0.829515</td>\n",
       "      <td>-0.580531</td>\n",
       "      <td>-0.472237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311505</td>\n",
       "      <td>-0.398070</td>\n",
       "      <td>-0.921178</td>\n",
       "      <td>-0.934378</td>\n",
       "      <td>-0.921178</td>\n",
       "      <td>0.241843</td>\n",
       "      <td>-0.994876</td>\n",
       "      <td>-0.731124</td>\n",
       "      <td>-0.921178</td>\n",
       "      <td>-0.974674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135819</th>\n",
       "      <td>13612</td>\n",
       "      <td>202004</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.921091</td>\n",
       "      <td>-0.999868</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.423602</td>\n",
       "      <td>-0.987052</td>\n",
       "      <td>-0.585561</td>\n",
       "      <td>-0.296143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333369</td>\n",
       "      <td>-0.402463</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.900766</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>0.229440</td>\n",
       "      <td>-0.992252</td>\n",
       "      <td>-0.717294</td>\n",
       "      <td>-0.880806</td>\n",
       "      <td>-0.961703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows × 914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno    DATE     mvel1      beta    betasq     chmom    dolvol  \\\n",
       "0        14593  202112  1.000000 -0.517544 -0.917058  0.083234  0.935369   \n",
       "1        14593  202109  0.850689 -0.513482 -0.915312  0.090737  0.932078   \n",
       "2        10107  202111  0.835936 -0.613520 -0.953011  0.035643  0.942327   \n",
       "3        10107  202112  0.830225 -0.612965 -0.952832  0.055599  0.935369   \n",
       "4        14593  202111  0.812219 -0.513624 -0.915374  0.004744  0.942327   \n",
       "...        ...     ...       ...       ...       ...       ...       ...   \n",
       "135687   60943  202004 -0.999988 -0.216641 -0.738566 -0.052503  0.530406   \n",
       "135744   18770  202004 -0.999993 -0.317152 -0.809281 -0.034219  0.335590   \n",
       "135768   18769  202004 -0.999994 -0.317152 -0.809281 -0.034219  0.453554   \n",
       "135818   90550  202004 -1.000000 -0.304787 -0.801182 -0.022320  0.479126   \n",
       "135819   13612  202004 -1.000000 -0.921091 -0.999868  0.017111  0.423602   \n",
       "\n",
       "         idiovol    indmom     mom1m  ...  invest*dfy  invest*svar  \\\n",
       "0      -0.766950 -0.369577 -0.175821  ...   -0.337816    -0.407181   \n",
       "1      -0.755616 -0.124235 -0.242029  ...   -0.335102    -0.408639   \n",
       "2      -0.801956 -0.176537 -0.100240  ...   -0.329061    -0.408331   \n",
       "3      -0.804418 -0.335381 -0.288292  ...   -0.329728    -0.406433   \n",
       "4      -0.764305 -0.243932 -0.224848  ...   -0.337273    -0.408725   \n",
       "...          ...       ...       ...  ...         ...          ...   \n",
       "135687 -0.803309 -0.627077 -0.622984  ...   -0.282093    -0.392161   \n",
       "135744 -0.802412 -0.585561 -0.382677  ...   -0.333369    -0.402463   \n",
       "135768 -0.802412 -0.585561 -0.299046  ...   -0.333369    -0.402463   \n",
       "135818 -0.829515 -0.580531 -0.472237  ...   -0.311505    -0.398070   \n",
       "135819 -0.987052 -0.585561 -0.296143  ...   -0.333369    -0.402463   \n",
       "\n",
       "        absacc*dp_sp  absacc*ep_sp  absacc*bm_sp  absacc*ntis  absacc*tbl  \\\n",
       "0          -0.870484     -0.809642     -0.864945     0.345191   -0.994172   \n",
       "1          -0.859405     -0.813324     -0.854991     0.361192   -0.996115   \n",
       "2          -0.893346     -0.848228     -0.886988     0.341805   -0.996143   \n",
       "3          -0.897154     -0.848841     -0.892756     0.328896   -0.995372   \n",
       "4          -0.865688     -0.808870     -0.857681     0.361447   -0.995143   \n",
       "...              ...           ...           ...          ...         ...   \n",
       "135687     -0.884515     -0.903855     -0.884515     0.230580   -0.992493   \n",
       "135744     -0.880806     -0.900766     -0.880806     0.229440   -0.992252   \n",
       "135768     -0.880806     -0.900766     -0.880806     0.229440   -0.992252   \n",
       "135818     -0.921178     -0.934378     -0.921178     0.241843   -0.994876   \n",
       "135819     -0.880806     -0.900766     -0.880806     0.229440   -0.992252   \n",
       "\n",
       "        absacc*tms  absacc*dfy  absacc*svar  \n",
       "0        -0.563798   -0.920009    -0.988168  \n",
       "1        -0.574823   -0.913856    -0.993453  \n",
       "2        -0.592870   -0.935504    -0.995051  \n",
       "3        -0.603814   -0.936481    -0.990605  \n",
       "4        -0.550015   -0.918778    -0.993767  \n",
       "...            ...         ...          ...  \n",
       "135687   -0.718565   -0.884515    -0.962895  \n",
       "135744   -0.717294   -0.880806    -0.961703  \n",
       "135768   -0.717294   -0.880806    -0.961703  \n",
       "135818   -0.731124   -0.921178    -0.974674  \n",
       "135819   -0.717294   -0.880806    -0.961703  \n",
       "\n",
       "[48000 rows x 914 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c00c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_top = features_top['0'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f31203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sin*ep_sp',\n",
       " 'beta*ep_sp',\n",
       " 'dy*dp_sp',\n",
       " 'ear*bm_sp',\n",
       " 'chpmia*ntis',\n",
       " 'chtx*tbl',\n",
       " 'cashdebt*ntis',\n",
       " 'chatoia*svar',\n",
       " 'ep*bm_sp',\n",
       " 'sic_67',\n",
       " 'stdacc*ep_sp',\n",
       " 'invest*dp_sp',\n",
       " 'roavol*tbl',\n",
       " 'realestate*ep_sp',\n",
       " 'lgr',\n",
       " 'roic*svar',\n",
       " 'retvol*tms',\n",
       " 'sgr',\n",
       " 'roaq*ep_sp',\n",
       " 'quick*bm_sp',\n",
       " 'chpmia',\n",
       " 'age*bm_sp',\n",
       " 'lgr*tbl',\n",
       " 'rsup*dfy',\n",
       " 'chempia*tbl',\n",
       " 'cinvest*tms',\n",
       " 'zerotrade',\n",
       " 'secured*dfy',\n",
       " 'pchsale_pchinvt*dfy',\n",
       " 'acc*bm_sp',\n",
       " 'ear*dfy',\n",
       " 'operprof*svar',\n",
       " 'lgr*dfy',\n",
       " 'retvol*ntis',\n",
       " 'ms*svar',\n",
       " 'pctacc*svar',\n",
       " 'sic_25',\n",
       " 'cfp*ep_sp',\n",
       " 'pchsale_pchinvt*bm_sp',\n",
       " 'absacc*tbl',\n",
       " 'stdcf*dfy',\n",
       " 'pchsale_pchrect*tbl',\n",
       " 'herf*dfy',\n",
       " 'pchgm_pchsale',\n",
       " 'baspread*tbl',\n",
       " 'herf*dp_sp',\n",
       " 'age*ntis',\n",
       " 'pchcurrat*ep_sp',\n",
       " 'pchcurrat',\n",
       " 'maxret',\n",
       " 'chempia*dp_sp',\n",
       " 'rd*tms',\n",
       " 'tb*ep_sp',\n",
       " 'sin*ntis',\n",
       " 'ms*ntis',\n",
       " 'pchcapx_ia*ntis',\n",
       " 'betasq',\n",
       " 'depr*dp_sp',\n",
       " 'egr*tms',\n",
       " 'pchsale_pchinvt',\n",
       " 'divi',\n",
       " 'absacc*ep_sp',\n",
       " 'chpmia*ep_sp',\n",
       " 'ear*ep_sp',\n",
       " 'rd_mve*dp_sp',\n",
       " 'baspread*ep_sp',\n",
       " 'chtx*dfy',\n",
       " 'orgcap*dfy',\n",
       " 'invest*tbl',\n",
       " 'pchquick*ep_sp',\n",
       " 'pchdepr*tbl',\n",
       " 'retvol*dfy',\n",
       " 'bm*dfy',\n",
       " 'tb*dfy',\n",
       " 'dolvol*ntis',\n",
       " 'baspread',\n",
       " 'beta*dfy',\n",
       " 'pchdepr*tms',\n",
       " 'mve_ia*tms',\n",
       " 'roavol*ntis',\n",
       " 'salerec*ep_sp',\n",
       " 'hire*svar',\n",
       " 'ms*ep_sp',\n",
       " 'sp*svar',\n",
       " 'rd*svar',\n",
       " 'ill*dfy',\n",
       " 'cfp_ia',\n",
       " 'sic_39',\n",
       " 'betasq*ntis',\n",
       " 'depr*dfy',\n",
       " 'quick*svar',\n",
       " 'pchsale_pchrect',\n",
       " 'mom1m*bm_sp',\n",
       " 'chempia*ntis',\n",
       " 'chmom*bm_sp',\n",
       " 'chinv*ntis',\n",
       " 'cashpr*tms',\n",
       " 'absacc',\n",
       " 'pchsale_pchxsga*ep_sp',\n",
       " 'tang*tbl',\n",
       " 'cash*ntis',\n",
       " 'ill*dp_sp',\n",
       " 'chempia*dfy',\n",
       " 'pchsale_pchrect*bm_sp',\n",
       " 'pchsaleinv*dfy',\n",
       " 'sic_42',\n",
       " 'sic_73',\n",
       " 'realestate*dp_sp',\n",
       " 'chmom*dp_sp',\n",
       " 'pchsale_pchrect*tms',\n",
       " 'sic_79',\n",
       " 'hire',\n",
       " 'pchsaleinv*ep_sp',\n",
       " 'realestate*bm_sp',\n",
       " 'pchgm_pchsale*ep_sp',\n",
       " 'chcsho*bm_sp',\n",
       " 'retvol*tbl',\n",
       " 'operprof*bm_sp',\n",
       " 'roic*bm_sp',\n",
       " 'lgr*bm_sp',\n",
       " 'stdacc*ntis',\n",
       " 'depr*svar',\n",
       " 'orgcap*dp_sp',\n",
       " 'turn',\n",
       " 'sic_24',\n",
       " 'sic_53',\n",
       " 'chinv*tbl',\n",
       " 'currat*tbl',\n",
       " 'gma*dfy',\n",
       " 'sp*tbl',\n",
       " 'pchdepr*dp_sp',\n",
       " 'tang*bm_sp',\n",
       " 'agr*ntis',\n",
       " 'pricedelay*dp_sp',\n",
       " 'tb*bm_sp',\n",
       " 'sic_13',\n",
       " 'divo*ep_sp',\n",
       " 'rd*tbl',\n",
       " 'mom6m*ntis',\n",
       " 'turn*dp_sp',\n",
       " 'idiovol*bm_sp',\n",
       " 'dolvol*bm_sp',\n",
       " 'mom12m*tbl',\n",
       " 'mvel1*tbl',\n",
       " 'pchquick*tbl',\n",
       " 'indmom*bm_sp',\n",
       " 'rsup*tbl',\n",
       " 'roaq*bm_sp',\n",
       " 'ear*ntis',\n",
       " 'chatoia*dfy',\n",
       " 'cfp*tbl',\n",
       " 'pchcapx_ia*ep_sp',\n",
       " 'cfp_ia*ntis',\n",
       " 'mom6m*tms',\n",
       " 'divo*bm_sp',\n",
       " 'sic_80',\n",
       " 'cashpr*ntis',\n",
       " 'rsup*svar',\n",
       " 'pchgm_pchsale*ntis',\n",
       " 'mom36m*dp_sp',\n",
       " 'zerotrade*bm_sp',\n",
       " 'agr*bm_sp',\n",
       " 'idiovol*dfy',\n",
       " 'chmom*dfy',\n",
       " 'ps*ntis',\n",
       " 'divi*bm_sp',\n",
       " 'pchdepr',\n",
       " 'sic_70',\n",
       " 'std_turn*ep_sp',\n",
       " 'chpmia*bm_sp',\n",
       " 'pchsale_pchrect*ep_sp',\n",
       " 'hire*tbl',\n",
       " 'agr*svar',\n",
       " 'beta*tbl',\n",
       " 'securedind*tms',\n",
       " 'pchcurrat*bm_sp',\n",
       " 'hire*dfy',\n",
       " 'chinv*svar',\n",
       " 'beta*ntis',\n",
       " 'gma*ep_sp',\n",
       " 'ms*dp_sp',\n",
       " 'ill*tms',\n",
       " 'stdacc*tbl',\n",
       " 'convind*ntis',\n",
       " 'divo',\n",
       " 'sic_72',\n",
       " 'mve_ia*tbl',\n",
       " 'mom36m*tms',\n",
       " 'ill*ep_sp',\n",
       " 'grcapx',\n",
       " 'dolvol*dfy',\n",
       " 'sgr*tbl',\n",
       " 'chempia*ep_sp',\n",
       " 'age*dfy',\n",
       " 'currat*bm_sp',\n",
       " 'sic_40',\n",
       " 'pchcapx_ia*tms',\n",
       " 'divo*tms',\n",
       " 'chcsho*svar',\n",
       " 'sgr*bm_sp',\n",
       " 'mom36m*tbl',\n",
       " 'pchdepr*ntis',\n",
       " 'roavol*tms',\n",
       " 'quick*dfy',\n",
       " 'sic_15',\n",
       " 'sgr*tms',\n",
       " 'lev*svar',\n",
       " 'sic_50',\n",
       " 'zerotrade*ntis',\n",
       " 'sic_58',\n",
       " 'depr*tbl',\n",
       " 'salecash*bm_sp',\n",
       " 'sic_49',\n",
       " 'saleinv*dfy',\n",
       " 'sgr*dfy',\n",
       " 'stdcf*dp_sp',\n",
       " 'cashpr*dfy',\n",
       " 'sic_26',\n",
       " 'stdcf*ntis',\n",
       " 'divi*ep_sp',\n",
       " 'roavol*dp_sp',\n",
       " 'aeavol*tms',\n",
       " 'pchdepr*dfy',\n",
       " 'sin*bm_sp',\n",
       " 'grcapx*ep_sp',\n",
       " 'ms*tbl',\n",
       " 'mom1m*dfy',\n",
       " 'bm*dp_sp',\n",
       " 'salecash*svar',\n",
       " 'retvol*bm_sp',\n",
       " 'stdacc*bm_sp',\n",
       " 'salecash*ep_sp',\n",
       " 'turn*bm_sp',\n",
       " 'baspread*svar',\n",
       " 'mve_ia*dfy',\n",
       " 'tb*dp_sp',\n",
       " 'nincr*tms',\n",
       " 'chatoia*bm_sp',\n",
       " 'gma*tms',\n",
       " 'currat*ep_sp',\n",
       " 'grcapx*ntis',\n",
       " 'pricedelay*ep_sp',\n",
       " 'convind*dfy',\n",
       " 'invest*tms',\n",
       " 'aeavol*bm_sp',\n",
       " 'sic_17',\n",
       " 'cash*dp_sp',\n",
       " 'betasq*ep_sp',\n",
       " 'bm*svar',\n",
       " 'gma*dp_sp',\n",
       " 'pchgm_pchsale*tms',\n",
       " 'sic_56',\n",
       " 'secured*dp_sp',\n",
       " 'chinv*dfy',\n",
       " 'grcapx*dp_sp',\n",
       " 'cfp',\n",
       " 'pchsaleinv*svar',\n",
       " 'idiovol*dp_sp',\n",
       " 'std_turn*dp_sp',\n",
       " 'saleinv*tms',\n",
       " 'chcsho*ntis',\n",
       " 'pchsaleinv*tbl',\n",
       " 'sp*ntis',\n",
       " 'ps*dp_sp',\n",
       " 'sic_57',\n",
       " 'roaq',\n",
       " 'betasq*dfy',\n",
       " 'chpmia*tms',\n",
       " 'convind*ep_sp',\n",
       " 'grltnoa*ntis',\n",
       " 'beta*bm_sp',\n",
       " 'quick*ntis',\n",
       " 'roic',\n",
       " 'chatoia*tms',\n",
       " 'dy',\n",
       " 'orgcap',\n",
       " 'ps*ep_sp',\n",
       " 'baspread*ntis',\n",
       " 'std_dolvol*ntis',\n",
       " 'maxret*tbl',\n",
       " 'salerec*tms',\n",
       " 'bm*tms',\n",
       " 'acc*tms',\n",
       " 'operprof*ntis',\n",
       " 'dy*tms',\n",
       " 'divo*dp_sp',\n",
       " 'sgr*dp_sp',\n",
       " 'chempia',\n",
       " 'tang*dfy',\n",
       " 'rd_mve*bm_sp',\n",
       " 'cfp*dfy',\n",
       " 'roaq*tms',\n",
       " 'orgcap*ntis',\n",
       " 'turn*ntis',\n",
       " 'chpmia*dfy',\n",
       " 'mom1m*svar',\n",
       " 'maxret*tms',\n",
       " 'chtx*tms',\n",
       " 'bm*ntis',\n",
       " 'maxret*bm_sp',\n",
       " 'sp*bm_sp',\n",
       " 'pchdepr*bm_sp',\n",
       " 'realestate*tbl',\n",
       " 'aeavol*ep_sp',\n",
       " 'stdacc*svar',\n",
       " 'sic_87',\n",
       " 'ep*ep_sp',\n",
       " 'dy*dfy',\n",
       " 'indmom*ntis',\n",
       " 'grltnoa*dp_sp',\n",
       " 'ms*dfy',\n",
       " 'egr*svar',\n",
       " 'herf*ntis',\n",
       " 'aeavol*tbl',\n",
       " 'rsup*tms',\n",
       " 'saleinv*svar',\n",
       " 'betasq*dp_sp',\n",
       " 'acc*dp_sp',\n",
       " 'aeavol*dp_sp',\n",
       " 'cashpr*bm_sp',\n",
       " 'nincr*svar',\n",
       " 'bm_ia*ntis',\n",
       " 'mom6m',\n",
       " 'ear*dp_sp',\n",
       " 'rsup*ep_sp',\n",
       " 'salecash*ntis',\n",
       " 'cashdebt*ep_sp',\n",
       " 'saleinv*bm_sp',\n",
       " 'mom1m*tms',\n",
       " 'ps*svar',\n",
       " 'mve_ia*ep_sp',\n",
       " 'sp*dfy',\n",
       " 'saleinv',\n",
       " 'lev*dp_sp',\n",
       " 'grcapx*bm_sp',\n",
       " 'chtx*svar',\n",
       " 'operprof*dfy',\n",
       " 'chatoia*tbl',\n",
       " 'lev*ntis',\n",
       " 'chmom*tms',\n",
       " 'realestate',\n",
       " 'chpmia*svar',\n",
       " 'dolvol*tbl',\n",
       " 'rd_sale*svar',\n",
       " 'hire*dp_sp',\n",
       " 'chatoia*ep_sp',\n",
       " 'pchdepr*svar',\n",
       " 'sic_62',\n",
       " 'lgr*dp_sp',\n",
       " 'chpmia*tbl',\n",
       " 'roic*ntis',\n",
       " 'stdcf',\n",
       " 'sic_12',\n",
       " 'sin*dp_sp',\n",
       " 'agr',\n",
       " 'baspread*tms',\n",
       " 'stdcf*svar',\n",
       " 'pctacc',\n",
       " 'roeq*dfy',\n",
       " 'aeavol*svar',\n",
       " 'pchcapx_ia*dfy',\n",
       " 'sic_47',\n",
       " 'secured*bm_sp',\n",
       " 'lev*tbl',\n",
       " 'pricedelay*dfy',\n",
       " 'cfp_ia*ep_sp',\n",
       " 'chmom',\n",
       " 'age*tms',\n",
       " 'lev*dfy',\n",
       " 'egr*ep_sp',\n",
       " 'lgr*ntis',\n",
       " 'realestate*dfy',\n",
       " 'roic*tbl',\n",
       " 'rd_sale*tbl',\n",
       " 'chempia*bm_sp',\n",
       " 'pchcapx_ia*svar',\n",
       " 'divo*dfy',\n",
       " 'roaq*svar',\n",
       " 'cash*ep_sp',\n",
       " 'mom12m*ntis',\n",
       " 'stdacc*tms',\n",
       " 'sic_31',\n",
       " 'aeavol*dfy',\n",
       " 'gma*ntis',\n",
       " 'bm_ia*bm_sp',\n",
       " 'nincr*bm_sp',\n",
       " 'chtx',\n",
       " 'sic_37',\n",
       " 'mom36m*dfy',\n",
       " 'zerotrade*dfy',\n",
       " 'cinvest*tbl',\n",
       " 'beta*dp_sp',\n",
       " 'dy*ep_sp',\n",
       " 'salerec*ntis',\n",
       " 'dy*tbl',\n",
       " 'dolvol',\n",
       " 'convind*dp_sp',\n",
       " 'pchsale_pchrect*dfy',\n",
       " 'agr*dfy',\n",
       " 'dolvol*ep_sp',\n",
       " 'gma',\n",
       " 'chtx*ep_sp',\n",
       " 'nincr*dp_sp',\n",
       " 'bm*ep_sp',\n",
       " 'bm_ia*tbl',\n",
       " 'divo*tbl',\n",
       " 'securedind*ntis',\n",
       " 'operprof*ep_sp',\n",
       " 'retvol',\n",
       " 'lgr*ep_sp',\n",
       " 'aeavol',\n",
       " 'mvel1*bm_sp',\n",
       " 'bm_ia*dp_sp',\n",
       " 'std_dolvol*ep_sp',\n",
       " 'idiovol*ntis',\n",
       " 'cashpr*tbl',\n",
       " 'cashpr*svar',\n",
       " 'sic_23',\n",
       " 'rd_sale*dfy',\n",
       " 'std_turn*svar',\n",
       " 'grcapx*tbl',\n",
       " 'egr*tbl',\n",
       " 'hire*tms',\n",
       " 'indmom*dfy',\n",
       " 'convind',\n",
       " 'sp*dp_sp',\n",
       " 'sic_64',\n",
       " 'pchsaleinv',\n",
       " 'ear*tms',\n",
       " 'sic_21',\n",
       " 'sic_32',\n",
       " 'chmom*ntis',\n",
       " 'dy*svar',\n",
       " 'lev*ep_sp',\n",
       " 'chcsho*dfy',\n",
       " 'acc*dfy',\n",
       " 'salerec*tbl',\n",
       " 'pchcapx_ia',\n",
       " 'pchsale_pchinvt*svar',\n",
       " 'chempia*svar',\n",
       " 'sgr*ntis',\n",
       " 'ep*tms',\n",
       " 'mom1m*tbl',\n",
       " 'sic_38',\n",
       " 'sic_61',\n",
       " 'ep*ntis',\n",
       " 'chtx*dp_sp',\n",
       " 'nincr*tbl',\n",
       " 'rd_sale*ntis',\n",
       " 'mom1m',\n",
       " 'ps*bm_sp',\n",
       " 'grltnoa',\n",
       " 'secured*ep_sp',\n",
       " 'sic_54',\n",
       " 'tb*ntis',\n",
       " 'roeq*dp_sp',\n",
       " 'pctacc*dfy',\n",
       " 'pctacc*tms',\n",
       " 'gma*tbl',\n",
       " 'bm*tbl',\n",
       " 'idiovol*tbl',\n",
       " 'securedind*bm_sp',\n",
       " 'cinvest*dfy',\n",
       " 'lev*bm_sp',\n",
       " 'invest',\n",
       " 'ep',\n",
       " 'mom36m*ntis',\n",
       " 'secured*ntis',\n",
       " 'sic_46',\n",
       " 'cfp*svar',\n",
       " 'chinv*bm_sp',\n",
       " 'chcsho*tms',\n",
       " 'ill*bm_sp',\n",
       " 'rd_mve*dfy',\n",
       " 'pchsale_pchrect*dp_sp',\n",
       " 'securedind*dfy',\n",
       " 'lev',\n",
       " 'std_turn*dfy',\n",
       " 'quick*tbl',\n",
       " 'pchcurrat*dp_sp',\n",
       " 'rd_sale',\n",
       " 'pchsaleinv*tms',\n",
       " 'hire*ntis',\n",
       " 'beta*svar',\n",
       " 'depr*bm_sp',\n",
       " 'zerotrade*dp_sp',\n",
       " 'ms',\n",
       " 'roavol*svar',\n",
       " 'pricedelay*bm_sp',\n",
       " 'operprof*dp_sp',\n",
       " 'rsup*bm_sp',\n",
       " 'pchsale_pchinvt*dp_sp',\n",
       " 'baspread*dfy',\n",
       " 'sic_83',\n",
       " 'turn*ep_sp',\n",
       " 'sic_36',\n",
       " 'securedind*svar',\n",
       " 'std_turn*tms',\n",
       " 'convind*bm_sp',\n",
       " 'pchsale_pchrect*ntis',\n",
       " 'salerec',\n",
       " 'pchquick*dfy',\n",
       " 'roeq*tms',\n",
       " 'dy*ntis',\n",
       " 'mve_ia*ntis',\n",
       " 'pricedelay',\n",
       " 'salecash*dfy',\n",
       " 'agr*dp_sp',\n",
       " 'currat*svar',\n",
       " 'roeq*svar',\n",
       " 'sin*dfy',\n",
       " 'rd*bm_sp',\n",
       " 'tang*svar',\n",
       " 'pricedelay*svar',\n",
       " 'sic_22',\n",
       " 'sp*ep_sp',\n",
       " 'turn*svar',\n",
       " 'turn*tbl',\n",
       " 'ep*svar',\n",
       " 'cinvest*ep_sp',\n",
       " 'rd',\n",
       " 'divi*svar',\n",
       " 'retvol*ep_sp',\n",
       " 'roic*ep_sp',\n",
       " 'pchsale_pchinvt*tms',\n",
       " 'agr*ep_sp',\n",
       " 'tb',\n",
       " 'bm_ia',\n",
       " 'salerec*dfy',\n",
       " 'pchgm_pchsale*bm_sp',\n",
       " 'chatoia*dp_sp',\n",
       " 'roic*dfy',\n",
       " 'roic*dp_sp',\n",
       " 'bm*bm_sp',\n",
       " 'mve_ia*bm_sp',\n",
       " 'herf*bm_sp',\n",
       " 'pchquick',\n",
       " 'mom6m*bm_sp',\n",
       " 'mom36m',\n",
       " 'ear*tbl',\n",
       " 'divi*tms',\n",
       " 'nincr*ep_sp',\n",
       " 'orgcap*ep_sp',\n",
       " 'cashdebt',\n",
       " 'acc*ntis',\n",
       " 'maxret*dfy',\n",
       " 'idiovol*svar',\n",
       " 'ill',\n",
       " 'cinvest*ntis',\n",
       " 'egr*dfy',\n",
       " 'sic_51',\n",
       " 'cashdebt*dp_sp',\n",
       " 'saleinv*ep_sp',\n",
       " 'quick',\n",
       " 'sic_45',\n",
       " 'sgr*ep_sp',\n",
       " 'mvel1*ep_sp',\n",
       " 'pctacc*bm_sp',\n",
       " 'retvol*svar',\n",
       " 'cash*tms',\n",
       " 'stdacc*dp_sp',\n",
       " 'orgcap*svar',\n",
       " 'roavol*dfy',\n",
       " 'divi*ntis',\n",
       " 'zerotrade*svar',\n",
       " 'retvol*dp_sp',\n",
       " 'mve_ia',\n",
       " 'operprof',\n",
       " 'tang*tms',\n",
       " 'secured*tms',\n",
       " 'chinv*tms',\n",
       " 'idiovol',\n",
       " 'betasq*bm_sp',\n",
       " 'aeavol*ntis',\n",
       " 'dolvol*tms',\n",
       " 'pchquick*svar',\n",
       " 'rd*dp_sp',\n",
       " 'cashdebt*tbl',\n",
       " 'mom6m*dfy',\n",
       " 'currat*ntis',\n",
       " 'cfp_ia*bm_sp',\n",
       " 'salecash',\n",
       " 'beta*tms',\n",
       " 'salecash*tms',\n",
       " 'hire*bm_sp',\n",
       " 'pchsale_pchinvt*ntis',\n",
       " 'mom6m*svar',\n",
       " 'pchsale_pchinvt*tbl',\n",
       " 'roaq*tbl',\n",
       " 'std_dolvol*svar',\n",
       " 'zerotrade*tms',\n",
       " 'bm_ia*tms',\n",
       " 'convind*tbl',\n",
       " 'age*tbl',\n",
       " 'cashdebt*svar',\n",
       " 'divi*dfy',\n",
       " 'tang*dp_sp',\n",
       " 'absacc*ntis',\n",
       " 'sic_30',\n",
       " 'cfp_ia*tbl',\n",
       " 'grltnoa*tbl',\n",
       " 'invest*ntis',\n",
       " 'egr',\n",
       " 'sic_75',\n",
       " 'salerec*svar',\n",
       " 'sic_28',\n",
       " 'mom12m*ep_sp',\n",
       " 'pchsale_pchinvt*ep_sp',\n",
       " 'ear*svar',\n",
       " 'secured*svar',\n",
       " 'depr*tms',\n",
       " 'grcapx*svar',\n",
       " 'pchcurrat*tms',\n",
       " 'mom12m*tms',\n",
       " 'bm_ia*svar',\n",
       " 'cashdebt*bm_sp',\n",
       " 'pchcurrat*ntis',\n",
       " 'grltnoa*ep_sp',\n",
       " 'tang*ntis',\n",
       " 'acc*tbl',\n",
       " 'mom12m',\n",
       " 'sic_1',\n",
       " 'tb*tms',\n",
       " 'stdcf*tbl',\n",
       " 'chatoia',\n",
       " 'cfp_ia*tms',\n",
       " 'sic_27',\n",
       " 'stdacc*dfy',\n",
       " 'maxret*svar',\n",
       " 'mvel1*ntis',\n",
       " 'herf*tbl',\n",
       " 'indmom*ep_sp',\n",
       " 'realestate*svar',\n",
       " 'sic_7',\n",
       " 'salerec*dp_sp',\n",
       " 'std_dolvol*dfy',\n",
       " 'agr*tbl',\n",
       " 'mvel1*dp_sp',\n",
       " 'invest*svar',\n",
       " 'pctacc*ntis',\n",
       " 'indmom*svar',\n",
       " 'salerec*bm_sp',\n",
       " 'std_dolvol*tms',\n",
       " 'tang',\n",
       " 'maxret*ep_sp',\n",
       " 'pchgm_pchsale*dfy',\n",
       " 'bm_ia*ep_sp',\n",
       " 'maxret*ntis',\n",
       " 'cinvest',\n",
       " 'rsup*ntis',\n",
       " 'rd*ep_sp',\n",
       " 'cfp*tms',\n",
       " 'mom36m*ep_sp',\n",
       " 'mvel1',\n",
       " 'pricedelay*tbl',\n",
       " 'sic_35',\n",
       " 'depr',\n",
       " 'pchgm_pchsale*svar',\n",
       " 'sin*svar',\n",
       " 'chcsho',\n",
       " 'sic_55',\n",
       " 'cfp_ia*dp_sp',\n",
       " 'ear',\n",
       " 'chinv*ep_sp',\n",
       " 'ep*dfy',\n",
       " 'rd_mve*ep_sp',\n",
       " 'chpmia*dp_sp',\n",
       " 'securedind*ep_sp',\n",
       " 'ill*tbl',\n",
       " 'securedind*dp_sp',\n",
       " 'egr*dp_sp',\n",
       " 'cashdebt*dfy',\n",
       " 'rd_sale*dp_sp',\n",
       " 'chcsho*dp_sp',\n",
       " 'invest*bm_sp',\n",
       " 'egr*bm_sp',\n",
       " 'betasq*svar',\n",
       " 'orgcap*bm_sp',\n",
       " 'pchgm_pchsale*dp_sp',\n",
       " 'quick*dp_sp',\n",
       " 'chempia*tms',\n",
       " 'rsup*dp_sp',\n",
       " 'absacc*tms',\n",
       " 'zerotrade*tbl',\n",
       " 'std_turn*bm_sp',\n",
       " 'pchsaleinv*bm_sp',\n",
       " 'divi*dp_sp',\n",
       " 'baspread*bm_sp',\n",
       " 'herf',\n",
       " 'std_turn*tbl',\n",
       " 'idiovol*tms',\n",
       " 'mom1m*dp_sp',\n",
       " 'roaq*dfy',\n",
       " 'dolvol*dp_sp',\n",
       " 'depr*ntis',\n",
       " 'rd_sale*tms',\n",
       " 'sic_16',\n",
       " 'pchquick*dp_sp',\n",
       " 'pchcurrat*tbl',\n",
       " 'currat*dp_sp',\n",
       " 'pchsale_pchxsga*tms',\n",
       " 'sic_34',\n",
       " 'pchquick*tms',\n",
       " 'cinvest*bm_sp',\n",
       " 'acc*ep_sp',\n",
       " 'divo*ntis',\n",
       " 'std_dolvol*tbl',\n",
       " 'grltnoa*bm_sp',\n",
       " 'bm_ia*dfy',\n",
       " 'pricedelay*ntis',\n",
       " 'mom12m*bm_sp',\n",
       " 'grltnoa*svar',\n",
       " 'mom1m*ntis',\n",
       " 'roeq',\n",
       " 'sin*tms',\n",
       " 'lgr*svar',\n",
       " 'std_dolvol',\n",
       " 'roavol*bm_sp',\n",
       " 'betasq*tms',\n",
       " 'mom1m*ep_sp',\n",
       " 'quick*ep_sp',\n",
       " 'sic_48',\n",
       " 'gma*bm_sp',\n",
       " 'roic*tms',\n",
       " 'orgcap*tbl',\n",
       " 'operprof*tbl',\n",
       " 'rd_mve*svar',\n",
       " 'acc*svar',\n",
       " 'cash*dfy',\n",
       " 'nincr',\n",
       " 'nincr*dfy',\n",
       " 'mom12m*dp_sp',\n",
       " 'rd_mve*tms',\n",
       " 'stdcf*bm_sp',\n",
       " 'betasq*tbl',\n",
       " 'salecash*dp_sp',\n",
       " 'chinv*dp_sp',\n",
       " 'tb*svar',\n",
       " 'herf*ep_sp',\n",
       " 'cashdebt*tms',\n",
       " 'secured*tbl',\n",
       " 'pchsale_pchxsga*bm_sp',\n",
       " 'herf*tms',\n",
       " 'ps*dfy',\n",
       " 'sic_20',\n",
       " 'roeq*ep_sp',\n",
       " 'currat*dfy',\n",
       " 'roeq*tbl',\n",
       " 'convind*tms',\n",
       " 'currat*tms',\n",
       " 'age',\n",
       " 'sic_10',\n",
       " 'saleinv*dp_sp',\n",
       " 'cash',\n",
       " 'cash*svar',\n",
       " 'convind*svar',\n",
       " 'indmom*dp_sp',\n",
       " 'ms*tms',\n",
       " 'absacc*svar',\n",
       " 'pchsale_pchxsga*dp_sp',\n",
       " 'rd_mve*tbl',\n",
       " 'currat',\n",
       " 'pchcurrat*svar',\n",
       " 'tang*ep_sp',\n",
       " 'cinvest*svar',\n",
       " 'absacc*dp_sp',\n",
       " 'roavol*ep_sp',\n",
       " 'mvel1*svar',\n",
       " 'rd_mve',\n",
       " 'pchcurrat*dfy',\n",
       " 'sp*tms',\n",
       " 'realestate*tms',\n",
       " 'cinvest*dp_sp',\n",
       " 'pchsale_pchxsga*svar',\n",
       " 'turn*tms',\n",
       " 'absacc*bm_sp',\n",
       " 'baspread*dp_sp',\n",
       " 'pchdepr*ep_sp',\n",
       " 'stdcf*tms',\n",
       " 'grltnoa*tms',\n",
       " 'rd*dfy',\n",
       " 'cashpr',\n",
       " 'securedind',\n",
       " 'pchquick*ntis',\n",
       " 'sic_29',\n",
       " 'roeq*bm_sp',\n",
       " 'pchquick*bm_sp',\n",
       " 'mom36m*bm_sp',\n",
       " 'cfp*ntis',\n",
       " 'sic_60',\n",
       " 'realestate*ntis',\n",
       " 'turn*dfy',\n",
       " 'age*dp_sp',\n",
       " 'operprof*tms',\n",
       " 'idiovol*ep_sp',\n",
       " 'pchsale_pchxsga*tbl',\n",
       " 'roeq*ntis',\n",
       " 'saleinv*tbl',\n",
       " 'indmom*tms',\n",
       " 'std_dolvol*dp_sp',\n",
       " 'divo*svar',\n",
       " 'pchsaleinv*ntis',\n",
       " 'gma*svar',\n",
       " 'depr*ep_sp',\n",
       " 'ms*bm_sp',\n",
       " 'ps',\n",
       " 'cfp_ia*dfy',\n",
       " 'chcsho*tbl',\n",
       " 'invest*dfy',\n",
       " 'ep*dp_sp',\n",
       " 'saleinv*ntis',\n",
       " 'maxret*dp_sp',\n",
       " 'chmom*ep_sp',\n",
       " 'sic_14',\n",
       " 'pchsaleinv*dp_sp',\n",
       " 'chcsho*ep_sp',\n",
       " 'pctacc*dp_sp',\n",
       " 'chmom*tbl',\n",
       " 'secured',\n",
       " 'sic_78',\n",
       " 'indmom',\n",
       " 'sic_44',\n",
       " 'sic_63',\n",
       " 'salecash*tbl',\n",
       " 'mom6m*ep_sp',\n",
       " 'std_turn',\n",
       " 'sic_52',\n",
       " 'mvel1*dfy',\n",
       " 'age*ep_sp',\n",
       " 'grcapx*dfy',\n",
       " 'orgcap*tms',\n",
       " 'stdacc',\n",
       " 'pchcapx_ia*tbl',\n",
       " 'sic_65',\n",
       " 'ill*svar',\n",
       " 'sic_33',\n",
       " 'rd_mve*ntis',\n",
       " 'absacc*dfy',\n",
       " 'chinv',\n",
       " 'mom6m*dp_sp',\n",
       " 'sic_82',\n",
       " 'chmom*svar',\n",
       " 'beta',\n",
       " 'dy*bm_sp',\n",
       " 'mvel1*tms',\n",
       " 'indmom*tbl',\n",
       " 'std_dolvol*bm_sp',\n",
       " 'sin',\n",
       " 'pchsale_pchxsga',\n",
       " 'egr*ntis',\n",
       " 'nincr*ntis',\n",
       " 'rd_sale*bm_sp',\n",
       " 'chtx*bm_sp',\n",
       " 'pctacc*ep_sp',\n",
       " 'pchsale_pchxsga*ntis',\n",
       " 'ill*ntis',\n",
       " 'cash*bm_sp',\n",
       " 'quick*tms',\n",
       " 'mom6m*tbl',\n",
       " 'roavol',\n",
       " 'mom12m*svar',\n",
       " 'pchcapx_ia*bm_sp',\n",
       " 'sic_59',\n",
       " 'divi*tbl',\n",
       " 'rd*ntis',\n",
       " 'securedind*tbl',\n",
       " 'pchsale_pchrect*svar',\n",
       " 'std_turn*ntis',\n",
       " 'sp',\n",
       " 'agr*tms',\n",
       " 'rd_sale*ep_sp',\n",
       " 'cfp_ia*svar',\n",
       " 'bm',\n",
       " 'ps*tbl',\n",
       " 'sic_99',\n",
       " 'hire*ep_sp',\n",
       " 'mom12m*dfy',\n",
       " 'chtx*ntis',\n",
       " 'sin*tbl',\n",
       " 'cashpr*ep_sp',\n",
       " 'grltnoa*dfy',\n",
       " 'pchcapx_ia*dp_sp',\n",
       " 'grcapx*tms',\n",
       " 'roaq*dp_sp',\n",
       " 'cash*tbl',\n",
       " 'lgr*tms',\n",
       " 'chatoia*ntis',\n",
       " 'stdcf*ep_sp',\n",
       " 'acc',\n",
       " 'mom36m*svar',\n",
       " 'ep*tbl',\n",
       " 'pricedelay*tms',\n",
       " 'invest*ep_sp',\n",
       " 'mve_ia*svar',\n",
       " 'cfp*dp_sp',\n",
       " 'cfp*bm_sp',\n",
       " 'pchsale_pchxsga*dfy',\n",
       " 'pchgm_pchsale*tbl',\n",
       " 'mve_ia*dp_sp',\n",
       " 'herf*svar',\n",
       " 'sgr*svar',\n",
       " 'pctacc*tbl',\n",
       " 'lev*tms',\n",
       " 'roaq*ntis',\n",
       " 'rsup',\n",
       " 'tb*tbl',\n",
       " 'zerotrade*ep_sp',\n",
       " 'age*svar',\n",
       " 'cashpr*dp_sp',\n",
       " 'ps*tms',\n",
       " 'dolvol*svar']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e2b6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 94 original features missing_info and correlation\n",
    "flat_corr = pd.read_csv(r\"flat_corr.csv\", index_col = 0)  # pairs correlation\n",
    "correlation_df_all = pd.read_csv(r\"correlation_df_all.csv\", index_col = 0)  # correlation with returns, p-values, and missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147e82f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pchquick</th>\n",
       "      <td>pchcurrat</td>\n",
       "      <td>0.974965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pchcurrat</th>\n",
       "      <td>pchquick</td>\n",
       "      <td>0.974965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>currat</td>\n",
       "      <td>0.971270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currat</th>\n",
       "      <td>quick</td>\n",
       "      <td>0.971270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>betasq</td>\n",
       "      <td>0.928150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agr</th>\n",
       "      <td>lgr</td>\n",
       "      <td>-0.603991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_dolvol</th>\n",
       "      <td>dolvol</td>\n",
       "      <td>-0.650953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolvol</th>\n",
       "      <td>std_dolvol</td>\n",
       "      <td>-0.650953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>absacc</td>\n",
       "      <td>-0.672360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absacc</th>\n",
       "      <td>acc</td>\n",
       "      <td>-0.672360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8742 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 1         0\n",
       "pchquick     pchcurrat  0.974965\n",
       "pchcurrat     pchquick  0.974965\n",
       "quick           currat  0.971270\n",
       "currat           quick  0.971270\n",
       "beta            betasq  0.928150\n",
       "...                ...       ...\n",
       "agr                lgr -0.603991\n",
       "std_dolvol      dolvol -0.650953\n",
       "dolvol      std_dolvol -0.650953\n",
       "acc             absacc -0.672360\n",
       "absacc             acc -0.672360\n",
       "\n",
       "[8742 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02c462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correlation</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>missing_per</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baspread</th>\n",
       "      <td>3.247846</td>\n",
       "      <td>6.964088e-192</td>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retvol</th>\n",
       "      <td>3.082055</td>\n",
       "      <td>7.123661e-173</td>\n",
       "      <td>0.070231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsup</th>\n",
       "      <td>-3.025205</td>\n",
       "      <td>2.455471e-125</td>\n",
       "      <td>25.136322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxret</th>\n",
       "      <td>2.610924</td>\n",
       "      <td>1.066366e-124</td>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>1.938797</td>\n",
       "      <td>1.605989e-54</td>\n",
       "      <td>22.261562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatoia</th>\n",
       "      <td>-0.037087</td>\n",
       "      <td>7.757253e-01</td>\n",
       "      <td>28.670843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ms</th>\n",
       "      <td>0.036791</td>\n",
       "      <td>7.736797e-01</td>\n",
       "      <td>26.150865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom12m</th>\n",
       "      <td>0.027462</td>\n",
       "      <td>8.090454e-01</td>\n",
       "      <td>6.393688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>securedind</th>\n",
       "      <td>-0.016067</td>\n",
       "      <td>8.974671e-01</td>\n",
       "      <td>22.246331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zerotrade</th>\n",
       "      <td>-0.011718</td>\n",
       "      <td>9.151283e-01</td>\n",
       "      <td>0.018736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Correlation        P-Value  missing_per\n",
       "Feature                                            \n",
       "baspread       3.247846  6.964088e-192     0.002055\n",
       "retvol         3.082055  7.123661e-173     0.070231\n",
       "rsup          -3.025205  2.455471e-125    25.136322\n",
       "maxret         2.610924  1.066366e-124     0.002055\n",
       "sp             1.938797   1.605989e-54    22.261562\n",
       "...                 ...            ...          ...\n",
       "chatoia       -0.037087   7.757253e-01    28.670843\n",
       "ms             0.036791   7.736797e-01    26.150865\n",
       "mom12m         0.027462   8.090454e-01     6.393688\n",
       "securedind    -0.016067   8.974671e-01    22.246331\n",
       "zerotrade     -0.011718   9.151283e-01     0.018736\n",
       "\n",
       "[94 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c669feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = correlation_df_all.index # original 94 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08681e4e",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "- Huber Loss Functions\n",
    "- Out-of-sample R-squared\n",
    "- Validation Function Using GridSerach Cross-validation\n",
    "- Huber Loss Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8042ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Huber objective function\n",
    "def huber_loss(actual, predicted, xi=1.35): \n",
    "    residual = actual - predicted\n",
    "    huber_loss = np.where(np.abs(residual) <= xi, residual**2, 2 * xi * (np.abs(residual) - 0.5 * xi))\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# Scoring function for out-of-sample R squared\n",
    "def r2_oos(actual, predicted):\n",
    "    # Convert input arrays to NumPy arrays\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    \n",
    "    # Calculate the sum of squared differences between actual and predicted\n",
    "    ss_residual = np.sum((actual - predicted) ** 2)\n",
    "    \n",
    "    # Calculate the total sum of squares (using the mean of actual values as the reference)\n",
    "    ss_total = np.sum((actual) ** 2)\n",
    "    \n",
    "    # Calculate R squared\n",
    "    r_squared = 1 - (ss_residual / ss_total)\n",
    "    \n",
    "    # Return the calculated R squared\n",
    "    return r_squared\n",
    "\n",
    "# Validation Function using GridSearchCV\n",
    "def val_fun_GridSearch(model, params: dict, X_train, y_train, X_val, y_val):\n",
    "    scorer = make_scorer(huber_loss, greater_is_better=False)\n",
    "#   grid_search = GridSearchCV(model, params, scoring=scorer, cv=5, verbose=0)\n",
    "    grid_search = GridSearchCV(model, params, scoring=scorer, verbose=0)\n",
    "    grid_search.fit(X_train, y_train, eval_set=[X_val, y_val])\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    best_ros = r2_oos(y_val, y_pred)\n",
    "\n",
    "    print('\\n' + '#' * 60)\n",
    "    print('Tuning process finished!!!')\n",
    "    print(f'The best setting is: {best_params}')\n",
    "    print(f'Out-of-sample R-squared on validation set is: {best_ros * 100:.2f}%')\n",
    "    print('#' * 60)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Validation Function\n",
    "def val_fun(model, params: dict, X_train, y_train, X_val, y_val):\n",
    "    best_score=float('inf')\n",
    "    # Hyperparameter tuning loop\n",
    "    for params_sample in ParameterGrid(params):\n",
    "        print(params_sample)\n",
    "        model = model.set_params(**params_sample)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        mse_val = huber_loss(y_val, y_val_pred)\n",
    "\n",
    "        # Check if this set of hyperparameters is the best so far\n",
    "        if mse_val < best_score:\n",
    "            best_score = mse_val\n",
    "            best_params = params_sample\n",
    "\n",
    "    # Train the final model with the best hyperparameters on the full training set\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the final model on the validation set\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    best_ros = r2_oos(y_val, y_pred)\n",
    "        \n",
    "    print('\\n' + '#' * 60)\n",
    "    print('Tuning process finished!!!')\n",
    "    print(f'The best setting is: {best_params}')\n",
    "    print(f'Out-of-sample R-squared on validation set is: {best_ros * 100:.2f}%')\n",
    "    print('#' * 60)\n",
    "\n",
    "    return best_model   \n",
    "\n",
    "\n",
    "# best_model = DecisionTreeRegressor()\n",
    "# best_model.set_params(**best_params)  # set best hyperparameters\n",
    "# best_model.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "def report_result(model, X_train, y_train, X_test, y_test):\n",
    "    print('\\n' + '#' * 60)\n",
    "    print(f\"In-sample R-squared is: {r2_oos(y_train, model.predict(X_train)) * 100:.2f}%\")\n",
    "    print(f\"Out-of-sample R-squared is: {r2_oos(y_test, model.predict(X_test)) * 100:.2f}%\")\n",
    "    print('#' * 60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eba1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gradient of Huber objective function with respect to predicted values\n",
    "def grad_huber_obj(actual, predicted, xi):\n",
    "    residual = actual - predicted\n",
    "    gradient = np.where(np.abs(residual) <= xi, -2 * residual, -2 * xi * np.sign(residual))\n",
    "    return gradient / len(actual)\n",
    "\n",
    "# Hessian of Huber objective function with respect to predicted values\n",
    "def hess_huber_obj(actual, predicted, xi):\n",
    "    residual = actual - predicted\n",
    "    hessian = np.where(np.abs(residual) <= xi, 2, 2 * xi)\n",
    "    return hessian / len(actual)\n",
    "\n",
    "# Example usage in a LightGBM objective\n",
    "def huber_objective(actual, predicted):\n",
    "    xi = 1.35\n",
    "    loss = huber_loss(actual, predicted, xi)\n",
    "    grad = grad_huber_obj(actual, predicted, xi)\n",
    "    hess = hess_huber_obj(actual, predicted, xi)\n",
    "    return grad, hess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a96d6",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # features is the list of feature names\n",
    "# # 900+ features\n",
    "\n",
    "# # Extract features and target from the datasets\n",
    "# train_dataset_top = train_dataset_top.dropna()\n",
    "# X_train = train_dataset_top[features_top]\n",
    "# y_train = train_dataset_top[\"RET\"]\n",
    "\n",
    "# val_dataset_top = val_dataset_top.dropna()\n",
    "# X_val = val_dataset_top[features_top]\n",
    "# y_val = val_dataset_top[\"RET\"]\n",
    "\n",
    "# test_dataset_top = test_dataset_top.dropna()\n",
    "# X_test = test_dataset_top[features_top]\n",
    "# y_test = test_dataset_top[\"RET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 94 Features\n",
    "\n",
    "# Extract features and target from the datasets\n",
    "train_dataset_top = train_dataset_top.dropna().head(8000)\n",
    "X_train_94char = train_dataset_top[chars]\n",
    "y_train_94char = train_dataset_top[\"RET\"]\n",
    "\n",
    "val_dataset_top = val_dataset_top.dropna().head(2000)\n",
    "X_val_94char = val_dataset_top[chars]\n",
    "y_val_94char = val_dataset_top[\"RET\"]\n",
    "\n",
    "test_dataset_top = test_dataset_top.dropna().head(2000)\n",
    "X_test_94char = test_dataset_top[chars]\n",
    "y_test_94char = test_dataset_top[\"RET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "050d4687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8adb5160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baspread</th>\n",
       "      <th>retvol</th>\n",
       "      <th>rsup</th>\n",
       "      <th>maxret</th>\n",
       "      <th>sp</th>\n",
       "      <th>sgr</th>\n",
       "      <th>chcsho</th>\n",
       "      <th>agr</th>\n",
       "      <th>pchsale_pchxsga</th>\n",
       "      <th>realestate</th>\n",
       "      <th>...</th>\n",
       "      <th>currat</th>\n",
       "      <th>cashdebt</th>\n",
       "      <th>beta</th>\n",
       "      <th>roaq</th>\n",
       "      <th>salecash</th>\n",
       "      <th>chatoia</th>\n",
       "      <th>ms</th>\n",
       "      <th>mom12m</th>\n",
       "      <th>securedind</th>\n",
       "      <th>zerotrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.775794</td>\n",
       "      <td>-0.763373</td>\n",
       "      <td>0.653197</td>\n",
       "      <td>-0.864516</td>\n",
       "      <td>-0.970232</td>\n",
       "      <td>-0.572842</td>\n",
       "      <td>0.649272</td>\n",
       "      <td>0.536005</td>\n",
       "      <td>-0.420397</td>\n",
       "      <td>-0.329844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980949</td>\n",
       "      <td>0.574771</td>\n",
       "      <td>-0.392488</td>\n",
       "      <td>0.475231</td>\n",
       "      <td>-0.984674</td>\n",
       "      <td>-0.009970</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.687399</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.769566</td>\n",
       "      <td>-0.783376</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>-0.881460</td>\n",
       "      <td>-0.964967</td>\n",
       "      <td>-0.562053</td>\n",
       "      <td>-0.463802</td>\n",
       "      <td>0.499835</td>\n",
       "      <td>-0.382289</td>\n",
       "      <td>-0.326746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966476</td>\n",
       "      <td>0.603289</td>\n",
       "      <td>-0.373529</td>\n",
       "      <td>0.473642</td>\n",
       "      <td>-0.990925</td>\n",
       "      <td>-0.134365</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.641790</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.732621</td>\n",
       "      <td>-0.766019</td>\n",
       "      <td>0.653197</td>\n",
       "      <td>-0.890465</td>\n",
       "      <td>-0.970232</td>\n",
       "      <td>-0.572842</td>\n",
       "      <td>0.649272</td>\n",
       "      <td>0.536005</td>\n",
       "      <td>-0.420397</td>\n",
       "      <td>-0.322474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980949</td>\n",
       "      <td>0.574771</td>\n",
       "      <td>-0.359465</td>\n",
       "      <td>0.475231</td>\n",
       "      <td>-0.984674</td>\n",
       "      <td>-0.009970</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.610022</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.768378</td>\n",
       "      <td>-0.790169</td>\n",
       "      <td>0.653197</td>\n",
       "      <td>-0.903153</td>\n",
       "      <td>-0.970232</td>\n",
       "      <td>-0.572842</td>\n",
       "      <td>0.647289</td>\n",
       "      <td>0.536005</td>\n",
       "      <td>-0.420397</td>\n",
       "      <td>-0.326229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980949</td>\n",
       "      <td>0.574771</td>\n",
       "      <td>-0.351181</td>\n",
       "      <td>0.475231</td>\n",
       "      <td>-0.984674</td>\n",
       "      <td>-0.009970</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.661673</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>-0.840305</td>\n",
       "      <td>-0.881242</td>\n",
       "      <td>0.663487</td>\n",
       "      <td>-0.961677</td>\n",
       "      <td>-0.970232</td>\n",
       "      <td>-0.572842</td>\n",
       "      <td>0.756123</td>\n",
       "      <td>0.536005</td>\n",
       "      <td>-0.420397</td>\n",
       "      <td>-0.341188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980949</td>\n",
       "      <td>0.574771</td>\n",
       "      <td>-0.377502</td>\n",
       "      <td>0.619684</td>\n",
       "      <td>-0.984674</td>\n",
       "      <td>-0.009970</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.681327</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21876</th>\n",
       "      <td>-0.840356</td>\n",
       "      <td>-0.745931</td>\n",
       "      <td>0.624726</td>\n",
       "      <td>-0.821193</td>\n",
       "      <td>-0.917991</td>\n",
       "      <td>-0.658815</td>\n",
       "      <td>-0.363673</td>\n",
       "      <td>0.719875</td>\n",
       "      <td>-0.397079</td>\n",
       "      <td>-0.321364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977731</td>\n",
       "      <td>0.516876</td>\n",
       "      <td>0.333884</td>\n",
       "      <td>0.334012</td>\n",
       "      <td>-0.999134</td>\n",
       "      <td>0.074719</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.791924</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21880</th>\n",
       "      <td>-0.741405</td>\n",
       "      <td>-0.818777</td>\n",
       "      <td>0.658134</td>\n",
       "      <td>-0.907629</td>\n",
       "      <td>-0.928931</td>\n",
       "      <td>-0.561901</td>\n",
       "      <td>-0.541649</td>\n",
       "      <td>0.538777</td>\n",
       "      <td>-0.328223</td>\n",
       "      <td>-0.322124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.982830</td>\n",
       "      <td>0.531087</td>\n",
       "      <td>-0.425142</td>\n",
       "      <td>0.458920</td>\n",
       "      <td>-0.967801</td>\n",
       "      <td>0.103442</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.708524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21886</th>\n",
       "      <td>-0.647245</td>\n",
       "      <td>-0.679284</td>\n",
       "      <td>0.649641</td>\n",
       "      <td>-0.814089</td>\n",
       "      <td>-0.943397</td>\n",
       "      <td>-0.822742</td>\n",
       "      <td>-0.598494</td>\n",
       "      <td>0.888456</td>\n",
       "      <td>-0.385645</td>\n",
       "      <td>-0.340619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971183</td>\n",
       "      <td>0.499886</td>\n",
       "      <td>-0.327442</td>\n",
       "      <td>0.367446</td>\n",
       "      <td>-0.988573</td>\n",
       "      <td>-0.078157</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.740568</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>-0.814822</td>\n",
       "      <td>-0.865493</td>\n",
       "      <td>0.648711</td>\n",
       "      <td>-0.945475</td>\n",
       "      <td>-0.979975</td>\n",
       "      <td>-0.644185</td>\n",
       "      <td>-0.430168</td>\n",
       "      <td>0.587497</td>\n",
       "      <td>-0.467313</td>\n",
       "      <td>-0.207147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942940</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>-0.426478</td>\n",
       "      <td>0.353250</td>\n",
       "      <td>-0.996898</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.658678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>-0.851884</td>\n",
       "      <td>-0.892041</td>\n",
       "      <td>0.647040</td>\n",
       "      <td>-0.932571</td>\n",
       "      <td>-0.965441</td>\n",
       "      <td>-0.625539</td>\n",
       "      <td>-0.445981</td>\n",
       "      <td>0.636674</td>\n",
       "      <td>-0.381184</td>\n",
       "      <td>0.421993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>0.539439</td>\n",
       "      <td>-0.496194</td>\n",
       "      <td>0.399874</td>\n",
       "      <td>-0.953064</td>\n",
       "      <td>0.032321</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.754413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       baspread    retvol      rsup    maxret        sp       sgr    chcsho  \\\n",
       "165   -0.775794 -0.763373  0.653197 -0.864516 -0.970232 -0.572842  0.649272   \n",
       "167   -0.769566 -0.783376  0.650936 -0.881460 -0.964967 -0.562053 -0.463802   \n",
       "176   -0.732621 -0.766019  0.653197 -0.890465 -0.970232 -0.572842  0.649272   \n",
       "178   -0.768378 -0.790169  0.653197 -0.903153 -0.970232 -0.572842  0.647289   \n",
       "181   -0.840305 -0.881242  0.663487 -0.961677 -0.970232 -0.572842  0.756123   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21876 -0.840356 -0.745931  0.624726 -0.821193 -0.917991 -0.658815 -0.363673   \n",
       "21880 -0.741405 -0.818777  0.658134 -0.907629 -0.928931 -0.561901 -0.541649   \n",
       "21886 -0.647245 -0.679284  0.649641 -0.814089 -0.943397 -0.822742 -0.598494   \n",
       "21888 -0.814822 -0.865493  0.648711 -0.945475 -0.979975 -0.644185 -0.430168   \n",
       "21892 -0.851884 -0.892041  0.647040 -0.932571 -0.965441 -0.625539 -0.445981   \n",
       "\n",
       "            agr  pchsale_pchxsga  realestate  ...    currat  cashdebt  \\\n",
       "165    0.536005        -0.420397   -0.329844  ... -0.980949  0.574771   \n",
       "167    0.499835        -0.382289   -0.326746  ... -0.966476  0.603289   \n",
       "176    0.536005        -0.420397   -0.322474  ... -0.980949  0.574771   \n",
       "178    0.536005        -0.420397   -0.326229  ... -0.980949  0.574771   \n",
       "181    0.536005        -0.420397   -0.341188  ... -0.980949  0.574771   \n",
       "...         ...              ...         ...  ...       ...       ...   \n",
       "21876  0.719875        -0.397079   -0.321364  ... -0.977731  0.516876   \n",
       "21880  0.538777        -0.328223   -0.322124  ... -0.982830  0.531087   \n",
       "21886  0.888456        -0.385645   -0.340619  ... -0.971183  0.499886   \n",
       "21888  0.587497        -0.467313   -0.207147  ... -0.942940  0.519968   \n",
       "21892  0.636674        -0.381184    0.421993  ... -0.989015  0.539439   \n",
       "\n",
       "           beta      roaq  salecash   chatoia    ms    mom12m  securedind  \\\n",
       "165   -0.392488  0.475231 -0.984674 -0.009970  0.75 -0.687399        -1.0   \n",
       "167   -0.373529  0.473642 -0.990925 -0.134365  0.75 -0.641790        -1.0   \n",
       "176   -0.359465  0.475231 -0.984674 -0.009970  0.75 -0.610022        -1.0   \n",
       "178   -0.351181  0.475231 -0.984674 -0.009970  0.75 -0.661673        -1.0   \n",
       "181   -0.377502  0.619684 -0.984674 -0.009970  0.75 -0.681327        -1.0   \n",
       "...         ...       ...       ...       ...   ...       ...         ...   \n",
       "21876  0.333884  0.334012 -0.999134  0.074719 -0.50 -0.791924        -1.0   \n",
       "21880 -0.425142  0.458920 -0.967801  0.103442  0.75 -0.708524         1.0   \n",
       "21886 -0.327442  0.367446 -0.988573 -0.078157  0.00 -0.740568         1.0   \n",
       "21888 -0.426478  0.353250 -0.996898  0.021250  0.25 -0.658678         1.0   \n",
       "21892 -0.496194  0.399874 -0.953064  0.032321  1.00 -0.754413         1.0   \n",
       "\n",
       "       zerotrade  \n",
       "165         -1.0  \n",
       "167         -1.0  \n",
       "176         -1.0  \n",
       "178         -1.0  \n",
       "181         -1.0  \n",
       "...          ...  \n",
       "21876       -1.0  \n",
       "21880       -1.0  \n",
       "21886       -1.0  \n",
       "21888       -1.0  \n",
       "21892       -1.0  \n",
       "\n",
       "[8000 rows x 94 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_94char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc7769",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51de07f",
   "metadata": {},
   "source": [
    "### Decision Tree with 900+ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     'max_depth':[3,5,10,20],\n",
    "#     'min_samples_split': [2],\n",
    "#     'min_samples_leaf': [1],\n",
    "#     'random_state':[42]\n",
    "# }\n",
    "\n",
    "\n",
    "# DecisionTree = val_fun(DecisionTreeRegressor(), params=params, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e36a5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cf65ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_result(model=DecisionTree, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bced6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6196c7b",
   "metadata": {},
   "source": [
    "### Decision Tree with 94 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "80054cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 42}\n",
      "Out-of-sample R-squared on validation set is: -18.03%\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "params = {\n",
    "    'max_depth':[3,5,10,20],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "DecisionTree = val_fun(DecisionTreeRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3095a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=3, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=3, random_state=42)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "453cb083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "In-sample R-squared is: 9.11%\n",
      "Out-of-sample R-squared is: 0.23%\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "report_result(model=DecisionTree, X_train=X_train_94char, y_train=y_train_94char, X_test=X_test_94char, y_test=y_test_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7cc06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89b86ce9",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f771c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23d0633",
   "metadata": {},
   "source": [
    "### Random Forest with 96 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "03fbacea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 42}\n",
      "{'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 42}\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 42}\n",
      "Out-of-sample R-squared on validation set is: -0.06%\n",
      "############################################################\n",
      "CPU times: total: 36.2 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# params = {\n",
    "#     'n_estimators': [300],\n",
    "#     'max_depth': [3, 6],\n",
    "#     'max_features': [30, 50, 100],\n",
    "#     'random_state': [42]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "RamdonForest = val_fun(RandomForestRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b38aed1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=3, max_features=30, n_estimators=300,\n",
       "                      random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=3, max_features=30, n_estimators=300,\n",
       "                      random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=3, max_features=30, n_estimators=300,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RamdonForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "695cc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "In-sample R-squared is: 9.62%\n",
      "Out-of-sample R-squared is: -0.97%\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "report_result(model=RamdonForest, X_train=X_train_94char, y_train=y_train_94char, X_test=X_test_94char, y_test=y_test_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892ff0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07721e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd71de4",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c18bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.1.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.10.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.23.5)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "baf4158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# Gradient of Huber objective function with respect to predicted values\n",
    "def grad_huber_obj(actual, predicted, xi):\n",
    "    residual = actual - predicted\n",
    "    gradient = np.where(np.abs(residual) <= xi, -2 * residual, -2 * xi * np.sign(residual))\n",
    "    return gradient / len(actual)\n",
    "\n",
    "# Hessian of Huber objective function with respect to predicted values\n",
    "def hess_huber_obj(actual, predicted, xi):\n",
    "    residual = actual - predicted\n",
    "    hessian = np.where(np.abs(residual) <= xi, 2, 2 * xi)\n",
    "    return hessian / len(actual)\n",
    "\n",
    "# Example usage in a LightGBM objective\n",
    "def huber_objective(actual, predicted):\n",
    "    xi = 1.35\n",
    "    loss = huber_loss(actual, predicted, xi)\n",
    "    grad = grad_huber_obj(actual, predicted, xi)\n",
    "    hess = hess_huber_obj(actual, predicted, xi)\n",
    "    return grad, hess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f601b8b",
   "metadata": {},
   "source": [
    "### LightGBM with 900+ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import os\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # params = {\n",
    "# #     'objective':[None, huber_objective],\n",
    "# #     'max_depth':[1,2,3,5,10],\n",
    "# #     'num_leaves': [10,30,50,80],\n",
    "# #     'n_estimators':[10,50,100, 500],\n",
    "# #     'random_state':[42],\n",
    "# #     'learning_rate':[0.01,.1]\n",
    "# # }\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     'objective':[None,huber_objective],\n",
    "#     'max_depth':[1,2,5,10,20],\n",
    "#     'n_estimators':[10,50,100,200,500,1000],\n",
    "#     'random_state':[12308],\n",
    "#     'learning_rate':[.01,.1]\n",
    "# }\n",
    "    \n",
    "    \n",
    "# # # Define hyperparameters\n",
    "# # params = {\n",
    "# #     'objective': 'regression',  # Note: The objective here is set to 'regression'\n",
    "# #     'metric': 'mse',\n",
    "# #     'boosting_type': 'gbdt',\n",
    "# #     'num_leaves': 31,\n",
    "# #     'learning_rate': 0.05,\n",
    "# #     'feature_fraction': 0.9\n",
    "# # }\n",
    "\n",
    "\n",
    "# LGBM = val_fun_GridSearch(lgb.LGBMRegressor(), params=params, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26967e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "396e095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_result(model=LGBM, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e5f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce2ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c8db2c",
   "metadata": {},
   "source": [
    "### LightGBM with 94 Feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "950eb6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 10, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 10, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 10, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 10, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'num_leaves': 10, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'num_leaves': 30, 'objective': None, 'random_state': 42}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'num_leaves': 30, 'objective': <function huber_objective at 0x00000255828D4790>, 'random_state': 42}\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Using self-defined objective function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21460\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.008123\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10, 'num_leaves': 10, 'objective': None, 'random_state': 42}\n",
      "Out-of-sample R-squared on validation set is: 2.80%\n",
      "############################################################\n",
      "CPU times: total: 29.7 s\n",
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# params = {\n",
    "#     'objective':[None, huber_objective],\n",
    "#     'max_depth':[1,2,5,10],\n",
    "#     'n_estimators':[10,50,100,200,500,1000],\n",
    "#     'random_state':[12308],\n",
    "#     'learning_rate':[.01,.1]\n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective':[None, huber_objective],\n",
    "    'max_depth':[1,2,5,10],\n",
    "    'num_leaves': [10,30],\n",
    "    'n_estimators':[10,50],\n",
    "    'random_state':[42],\n",
    "    'learning_rate':[.1]\n",
    "}\n",
    "\n",
    "# # Define hyperparameters\n",
    "# params = {\n",
    "#     'objective': 'regression',  # Note: The objective here is set to 'regression'\n",
    "#     'metric': 'mse',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9\n",
    "# }\n",
    "\n",
    "# LGBM = val_fun_GridSearch(lgb.LGBMRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)\n",
    "LGBM = val_fun(lgb.LGBMRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7e06a61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(max_depth=1, n_estimators=10, num_leaves=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(max_depth=1, n_estimators=10, num_leaves=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(max_depth=1, n_estimators=10, num_leaves=10, random_state=42)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90febaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "In-sample R-squared is: 4.21%\n",
      "Out-of-sample R-squared is: 0.18%\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "report_result(model=LGBM, X_train=X_train_94char, y_train=y_train_94char, X_test=X_test_94char, y_test=y_test_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3ce06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88cbb9e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c486149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "     --------------------------------------- 99.8/99.8 MB 18.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a1d00",
   "metadata": {},
   "source": [
    "### XGBoost with 94 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7be6aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'objective': None, 'subsample': 0.8}\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'objective': <function huber_objective at 0x00000255828D4790>, 'subsample': 0.8}\n",
      "Out-of-sample R-squared on validation set is: 4.77%\n",
      "############################################################\n",
      "CPU times: total: 8min 27s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    'objective': [None, huber_objective],  # Using Huber loss for regression\n",
    "    'max_depth': [2,3,5,10],  # Adjust the maximum depth of the trees\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'n_estimators': [50, 100, 200]  # Number of boosting rounds\n",
    "}\n",
    "\n",
    "# XGBoost = val_fun_GridSearch(xgb.XGBRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)\n",
    "XGBoost = val_fun(xgb.XGBRegressor(), params=params, X_train=X_train_94char, y_train=y_train_94char, X_val=X_val_94char, y_val=y_val_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fdbdab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None,\n",
       "             objective=&lt;function huber_objective at 0x00000255828D4790&gt;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None,\n",
       "             objective=&lt;function huber_objective at 0x00000255828D4790&gt;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None,\n",
       "             objective=<function huber_objective at 0x00000255828D4790>, ...)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2a8a7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "In-sample R-squared is: 1.53%\n",
      "Out-of-sample R-squared is: 2.72%\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "report_result(model=XGBoost, X_train=X_train_94char, y_train=y_train_94char, X_test=X_test_94char, y_test=y_test_94char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d3225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef77cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbf181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479a099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
